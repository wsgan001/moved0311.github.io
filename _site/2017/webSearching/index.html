<!DOCTYPE html>
<html lang="en-us">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/javascript">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath:[ ['$$','$$'], ['\\[','\\]'] ], 
        processEscapes: true
    }        
})
</script>

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      WebSearching &middot; Note
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/custom.css">
  
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">


</head>


  <body class="theme-base-0b">

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Note
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/Archive/">Archives</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about/">About</a>
          
        
      
        
      
        
          
        
      
			
	
	<li>
		<a href="https://github.com/moved0311">
		   <span class="fa-stack fa-lg">
              <i class="fa fa-circle fa-stack-2x"></i>
              <i class="fa fa-github fa-stack-1x icon"></i>
          </span>
		</a>
	</li>
	

	
	  <li>
		<a href="https://www.facebook.com/100000329876068">
		  <span class="fa-stack fa-lg">
              <i class="fa fa-circle fa-stack-2x"></i>
              <i class="fa fa-facebook fa-stack-1x icon"></i>
          </span>
		</a>
	  </li>
	

    </nav>

    <p>@ 2018</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">WebSearching</h1>
  <span class="post-date">07 Mar 2017</span>
  <hr />

<p>課本: <a href="https://nlp.stanford.edu/IR-book/">Introduction to Information Retrieval</a></p>

<hr />

<h1 id="ch8-evaluation-in-information-retrival">CH8 Evaluation in information retrival</h1>
<p>評量search engine好壞</p>
<ol>
  <li>搜到的index</li>
  <li>搜尋速度</li>
  <li>二氧化碳排放量</li>
  <li>和搜尋相關程度</li>
</ol>

<!--more-->
<p>相關程度</p>
<ol>
  <li>benchmark資料集</li>
  <li>benchmark queries(問句)</li>
  <li>文章是否相關的標記(ground truths)</li>
</ol>

<p>queries和information need有落差
想找的東西,不會下key word</p>

<p><strong>Precision (P)</strong></p>
<blockquote>
  <p>Precision = <script type="math/tex">\frac{檢索到相關物件的數量}{物件總數}\</script></p>
</blockquote>

<p><strong>Recall (R)</strong></p>
<blockquote>
  <p>Recall = <script type="math/tex">\frac{檢索到相關物件的數量}{相關物件總數}\</script></p>
</blockquote>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>Relevant</td>
      <td>Nonrelevent</td>
    </tr>
    <tr>
      <td>Retrieved</td>
      <td>true positive(tp)</td>
      <td>false positive(fp)</td>
    </tr>
    <tr>
      <td>Not retrieved</td>
      <td>false negatives(fn)</td>
      <td>true negatives(tn)</td>
    </tr>
  </tbody>
</table>

<p>true positive(tp) 機器判斷+且為真<br />
false positive(fp)機器判斷+但是是假<br />
false negatives(fn) 機器判斷-但判斷錯<br />
true negatives(tn)  機器判斷-且判斷對</p>

<p>P = <script type="math/tex">\frac{tp}{tp+fp}\</script></p>

<p>R = <script type="math/tex">\frac{tp}{tp+fn}\</script></p>

<p><strong>Accuracy vs Precision</strong> <br />
accuracy = <script type="math/tex">\frac{tp+tn}{tp+fp+fn+tn}\</script></p>
<blockquote>
  <p>accuracy不適合用在information retrieval,
因為通常Nonrelevant會非常的大,tn項非常大除分母fn和tn都非常大結果會趨近於1,所以才用Precision和Recall作為依據</p>
</blockquote>

<p><strong>調和平均數(harmonic mean)</strong></p>
<blockquote>
  <p>H = <script type="math/tex">\frac{n}{\frac{1}{x_1}+\frac{1}{x_2}+..+\frac{1}{x_n}}\</script></p>
</blockquote>

<p>Precision/Recall Tradoff
使用調和平均數計算Precision和Recall的Tradoff<br />
量測的數值稱做F measure,α和1-α分別為P和R的權重,一般是取α=0.5<br />
(P和R重要程度相同)</p>
<blockquote>
  <p>F = <script type="math/tex">\frac{1}{\alpha\frac{1}{P}+(1-\alpha)\frac{1}{R}} = \frac{(\beta^2+1)PR}{\beta^2P+R}\</script> where <script type="math/tex">\ \beta^2=\frac{1-\alpha}{\alpha}\</script></p>
</blockquote>

<h4 id="ranked-evalution">Ranked Evalution</h4>
<p>P,R,F都是unordered(沒有等級)<br />
一個query會有一個Precision/Recall圖    <br />
使用內插法(interpolated)可以得到一張較平滑的P-R圖<br />
(和機器學習ROC curve相似)<br />
P-R curve的面積越大效能越佳(代表Precision掉越慢)</p>

<p><strong>內插法</strong></p>
<blockquote>
  <script type="math/tex; mode=display">\ p_{interp}= \max\limits_{r'\ge r}\ p(r{'})\</script>
</blockquote>

<p>r代表recall,
作法是從目前往後找最高的點向前填平,並重新畫P-R圖</p>

<p><strong>Mean Average Precision(MAP)</strong></p>
<blockquote>
  <script type="math/tex; mode=display">\ MAP(Q) = \frac{1}{|Q|}\sum^{|Q|}_{j=1}\frac{1}{m_j}\sum_{k=1}^{m_j}Precision(R_{jk})\</script>
</blockquote>

<p>第一個sum算query平均<br />
第二個sum算precision平均</p>

<p><strong>Precision at k</strong><br />
第k個搜索結果的Precision</p>

<p><strong>R-Precision</strong><br />
文件中總共有R篇相關文章,以R作為cut-off,計算Precision<br />
e.g. 總共有100篇文章,其中10篇是相關的<br />
且搜尋結果是:RNRNN RRNNN RNNRR ….<br />
R=10(只看RNRNN RRNNN)計算Precision<br />
R-Precision = 0.4</p>

<p><strong>Normalized Discounted Cumulative Gain(NDCG)</strong>  <br />
作者：Kalervo Jarvelin, Jaana Kekalainen(2002)</p>
<blockquote>
  <p>用來衡量ranking quality</p>
</blockquote>

<p>e.g.<br />
G = &lt;3,2,3,0,0,1,2,2,3,0,…&gt; <br />
G表示一個搜索的結果(3高度相關, 0沒關係)<br />
步驟:</p>
<ol>
  <li>Cumulative Gain(CG)
    <blockquote>

      <script type="math/tex; mode=display">% <![CDATA[
\
 CG[i] = \left\{\begin{matrix}
 G[1], &if\ i=1 \\ 
 CG[i-1]+G[i], &otherwise 
 \end{matrix}\right.
 \ %]]></script>
    </blockquote>

    <p>目前項+＝前一項(做成一個遞增的函數)<br />
 CG’=&lt;3,5,8,8,8,9,11,13,16,16,…&gt;</p>
  </li>
  <li>Discounted Cumulative Gain(DCG)
    <blockquote>

      <script type="math/tex; mode=display">% <![CDATA[
\
 DCG[i]=\left\{\begin{matrix}
 G[i], & if\ i=1\\ 
 DCG[i-1]+G[i]/log_b\ i, & otherwise
 \end{matrix}\right.
 \ %]]></script>
    </blockquote>

    <p>DCG’=&lt;3,5,6.89,6.89,6.89,7.28,7.99,8.66,9.61,9.61,…&gt; if b=2<br />
 i代表排名,對排名做懲罰(除log<sub>2</sub> i),排名越後面懲罰越重<br />
 代表如果搜尋的結果很差,和理想的排序分數會相差很多</p>
  </li>
  <li>Normalized Discounted Cumulative Gain(NDCG)<br />
 理想的搜索結果I=&lt;3,3,3,2,2,2,1,1,1,1,0,0,0,…&gt;(高度相關的排越前面) <br />
 理想搜索結果DCGI=&lt;3,6,7.89,8.89,9.75,10.52,10.88,11.21,…&gt;<br />
 nDCG<sub>n</sub> = <script type="math/tex">\ \frac{DCG_{n}}{IDCG_{n}}(\frac{相關程度排序}{理想相關程度},做正規化)\</script><br />
 NDCG=&lt;1,0.83,0.87,0.77,0.70,0.69,0.73,0.77,…&gt;</li>
</ol>

<p><strong>benchmark 資料集</strong></p>
<ol>
  <li>Cranfield</li>
  <li>TREC(nist.gov)<br />
 Ad-hoc 資料集(1992-1999)</li>
  <li>GOV2
 2500萬篇文章</li>
  <li>NICIR
 cross-language IR</li>
  <li>Cross Language Evaluation</li>
  <li>REUTERS</li>
</ol>

<p><strong>標記資料準則</strong> <br />
Kappa measure</p>
<blockquote>
  <p>標記資料是否一致的衡量標準,若標記不一致資料中就沒有truth</p>
</blockquote>

<p>Kappa計算公式</p>
<blockquote>
  <script type="math/tex; mode=display">\ \kappa = \frac{P(A)-P(E)}{1-P(E)}\</script>
</blockquote>

<p><strong>搜尋結果的呈現 Result Summaries</strong></p>
<ul>
  <li>搜尋結果呈現：10 blue link</li>
  <li>搜尋結果下方文字說明分為Static和Dynamic
Static:固定抽前50個字
Dynamic:利用nlp技術,根據搜索關鍵字動態做變化</li>
  <li>quicklinks<br />
底下多的連結</li>
</ul>

<hr />

<h1 id="ch6-model">CH6 Model</h1>
<ul>
  <li>Vector Space Model</li>
  <li>Probabilistic Information Retrieval</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>Empirical IR</td>
      <td>Model-based IR</td>
    </tr>
    <tr>
      <td>暴力法</td>
      <td>有理論模型</td>
    </tr>
    <tr>
      <td>heuristic</td>
      <td>數學假設</td>
    </tr>
    <tr>
      <td>難推廣到其他問題</td>
      <td>容易推廣到其他問題</td>
    </tr>
  </tbody>
</table>

<p><strong>IR model歷史</strong></p>
<ul>
  <li>1960<br />
  第一個機率模型</li>
  <li>1970   <br />
  vector space model(75)  <br />
  classic probabilistic model(76)</li>
  <li>1980<br />
  non-class logic model(86)</li>
  <li>1990<br />
  TREC benchmark <br />
  BM25/Okapi(94)<br />
  google成立(96)  <br />
  Language model(98)</li>
  <li>2000<br />
  Axiomatic model(04)<br />
  Markov Random Field(05)   <br />
  Learning to rank(05)</li>
</ul>

<p><strong>Vector space</strong></p>

<table>
  <tbody>
    <tr>
      <td>Vocabulary</td>
      <td>V = { <script type="math/tex">w_1,w_2,w_3,...w_v</script> }</td>
    </tr>
    <tr>
      <td>Query</td>
      <td>q =<script type="math/tex">\{q_1,q_2,...,q_m\}</script></td>
    </tr>
    <tr>
      <td>Document 文章</td>
      <td><script type="math/tex">{d_i} = \{  w_1,w_2,...  \}</script></td>
    </tr>
    <tr>
      <td>Collection文章集合</td>
      <td>C = { <script type="math/tex">d_1,d_2,d_3,...</script> }</td>
    </tr>
    <tr>
      <td>R(q) query的集合</td>
      <td>R(q) ⊂ C</td>
    </tr>
  </tbody>
</table>

<p><strong>目標是找到近似query的集合</strong><br />
策略:</p>
<ol>
  <li>Document select<br />
 挑文件如果是相關就收到集合<br />
 absolute relevance(系統必須決定是相關還是不相關)</li>
  <li>Document ranking<br />
 query的結果&gt;threshold 就收進去  <br />
 relative relevance(不必是1或0,相近到一定程度就收進集合)</li>
</ol>

<p><strong>Probability Ranking Principle(PRP)</strong></p>
<blockquote>
  <p>Robertson (1977)<br />
相似度量測函數f滿足,<br />
<script type="math/tex">f(q,d_1) > f(q,d_2)\quad iff\quad p(Rel|q,d_1) > p(Rel|q,d_2)</script>  <br />
f()值越大表示有越大的機率越相似</p>
</blockquote>

<p><strong>Relevance流派</strong></p>
<ul>
  <li>Similarity 相似度<br />
   Vector space model(Salton et al,75)</li>
  <li>Probability of relevance 機率模型
  Classical probaility Model(Robertson&amp;Sparck Jones,76)<br />
  Learning to Rank(Joachims,02, Berges et al,05)</li>
  <li>Probability inference 機率推論</li>
</ul>

<p><strong>Vector Space Model(VSM)</strong><br />
將query和document表示成向量形式(similar representation)<br />
假設Relevance(d,q) = similar(d,q)<br />
利用cosine算相似度(1 ~ -1) <br />
high dimension(index的維度通常在10萬左右) <br />
good dimension -&gt; orthogonal<br />
(好的維度切割應該是維度間彼此獨立(orthogonal),<br />
但是通常很困難,例如nccu後面接university的機率很高)<br />
VSM優點: Empirically effective,直觀, 實作容易</p>

<p><strong>VectorSpace範例程式</strong><br />
<a href="http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html">Building a Vector Space Search Engine in Python</a></p>

<p>大致步驟</p>
<ul>
  <li>將所有文章使用join()成為一個string包含所有文章內容</li>
  <li>做string clean去除<code class="highlighter-rouge">.</code> <code class="highlighter-rouge">,</code> <code class="highlighter-rouge">多餘空白</code>,並轉為小寫</li>
  <li>將clean好的string利用空白切分成words array,丟到<a href="https://tartarus.org/martin/PorterStemmer/">Porter stem</a>(去除字尾)
    <blockquote>
      <p>Porter Stemming Algorithm<br />
    作者:Martin Porter(2006)</p>
    </blockquote>
  </li>
  <li>刪除重複的word,使用set讓出現的word唯一</li>
  <li>得到所有整理完的words,做成index(將每個word編號),類似字典的概念</li>
  <li>將每篇文章分別建立自己的vector,並統計每個word出現的次數(term frequecy)</li>
  <li>將輸入的query做成vector</li>
  <li>利用兩個向量做cosine計算相關程度</li>
</ul>

<p><strong>相似度計算</strong></p>
<ol>
  <li>Cosine Similarity
    <blockquote>
      <p>cosine = <script type="math/tex">\frac{V_1 \cdot V_2}{\|V_1\|\|V_2\|}</script></p>
    </blockquote>
  </li>
  <li>Jaccard Similarity
    <blockquote>
      <p>相似度 = <script type="math/tex">\frac{交集}{聯集}</script></p>
    </blockquote>
  </li>
</ol>

<p><strong>TF-IDF Weighting</strong></p>
<ul>
  <li>TF(Term Frequency)<br />
  word count,單純統計字數出現頻率</li>
  <li>IDF(Inverse Document Frequency)(反向的TF)<br />
  字的獨特性,如果某些字在很多篇文章出現次數都很高(例如:the,a,to,…) <br />
  IDF值就會很低(沒有鑑別度)	<br />
  IDF(t) = 1 + log(n/k)  (n:篇數,k:字出現次數)<br />
  例如文章總數是1000(n=1000),所有文章都有出現cat(k=1000),<br />
  IDF = 1 + log(1000/1000) = 1<br />
  如果只有1篇文章有出現cat,<br />
  IDF = 1 + log(1000/1) = 4</li>
</ul>

<p>TF-IDF計算方法:</p>
<blockquote>
  <p>weight(t,d) = TF(t,d) * IDF(t)</p>
</blockquote>

<p><strong>TF-IDF範例程式</strong><br />
<a href="http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/">Tutorial: Finding Important Words in Text Using TF-IDF</a></p>

<p><br /></p>
<hr />

<h1 id="ch11-probabilistic-information-retrieval">Ch11 Probabilistic Information Retrieval</h1>

<p><strong>Probability theory</strong></p>
<ul>
  <li>Joint probability <br />
  <script type="math/tex">P(A\cap B)</script></li>
  <li>Conditional probability  <br />
  <script type="math/tex">P(A | B)</script><br />
  probability of A given that event B occurred.</li>
  <li>Chain rule <br />
  <script type="math/tex">P(A,B) = P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)</script></li>
  <li>Partition rule<br />
  <script type="math/tex">P(B) = P(A,B) + P(\bar{A},B)</script></li>
</ul>

<p><a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">Chain rule example wiki</a></p>
<blockquote>
  <p>有兩個甕第一個甕放1個黑球2個白球,第二個甕放1個黑球3個白球<br />
事件A是選到第一個甕,事件B是選到白球<br />
<script type="math/tex">P(B|A)= \frac{2}{3}</script> 在選到第一個甕的情況下拿到白球<br />
<script type="math/tex">P(A,B)=P(B|A)P(A)=\frac{2}{3} \times \frac{1}{2}</script></p>
</blockquote>

<p><strong>Bayes' Rule</strong><br />
<script type="math/tex">P(A|B) = \frac{P(B|A)P(A)}{P(B)}</script></p>

<p>from chain rule: <script type="math/tex">P(A|B)P(B) = P(B|A)P(A)</script></p>
<div class="language-text highlighter-rouge"><pre class="highlight"><code>P(A)    : 事前機率(prior probability)  
P(A|B)  : 事後機率(postior probability)  
P(B|A)  : likelihood   

The term likelihood is just a synonym of probability.   
</code></pre>
</div>
<p><strong>Odds</strong><br />
<script type="math/tex">O(A) = \frac{P(A)}{P(\bar{A})} = \frac{P(A)}{1-P(A)}</script></p>
<blockquote>
  <p>an event provide a kind of multiplier for how probabilities change.</p>
</blockquote>

<p><strong>Probability of Relevance</strong></p>
<ul>
  <li>Random variables:
    <ul>
      <li>query Q</li>
      <li>document D</li>
      <li>relevance R ∈ {0,1} <br />
(1:相關,0:不相關)</li>
    </ul>
  </li>
  <li>Goal: P(R=1|Q,D) to rank relevant<br />
  利用query和document相似度的機率做排名</li>
</ul>

<p><strong>Refining P(R=1|Q,D) Methods</strong></p>
<ol>
  <li>Conditional Models(Discriminative Models)
    <ul>
      <li>利用各種方法找出機率P = f(x)</li>
      <li>利用資料訓練參數</li>
      <li>利用model去排列未知的document<br />
 e.g. Learning to rank,類神經網路,…</li>
    </ul>
  </li>
  <li>Generative Models
    <ul>
      <li>compute the odd of O(R=1|Q,D) using Bayes' rules<br />
 先找出資料的分佈再做預測</li>
      <li>How to define P(Q,D|R)
        <ul>
          <li>Document generation: P(Q,D|R)=P(D|Q,R)P(Q|R) <br />
  query放到條件 (e.g RSJ model)</li>
          <li>Query generation: P(Q,D|R)=P(Q|D,R)P(D|R) <br />
  document放到條件 (e.g language model)</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h4 id="rsj-model-binary-independence-model">RSJ Model <a href="https://en.wikipedia.org/wiki/Binary_Independence_Model">(Binary Independence Model)</a></h4>
<p>利用<strong>Odd</strong>值做ranking的依據:<br />
<script type="math/tex">O(R|D,Q) = \frac{P(R=1|D,Q)}{P(R=0|D,Q)} = \frac{ \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q)} }{ \frac{P(D|Q,R=0)P(R=0|Q)}{P(D|Q)} } = \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q,R=0)P(R=0|Q)} \quad(1)</script></p>

<blockquote>
  <p><script type="math/tex">\frac{P(R=1|Q)}{P(R=0|Q)}</script> 
對document ranking沒有影響,視為常數</p>
</blockquote>

<script type="math/tex; mode=display">\frac{P(D|Q,R=1)}{P(D|Q,R=0)} = \prod_{t=1}^{M} \frac{P(D_t|Q,R=1)}{P(D_t|Q,R=0)} \quad(2)</script>

<blockquote>
  <p>將document拆成多個獨立的document term連乘積,且<script type="math/tex">D_t \in \{0,1\}</script></p>
</blockquote>

<script type="math/tex; mode=display">O(R|D,Q) = O(R|Q) \cdot \prod_{t=1}^{M} \frac{P(D_t|R=1,Q)}{P(D_t|R=0,Q)} \quad(3)</script>

<blockquote>
  <p>(2)代入(1)可以整理出(3)</p>
</blockquote>

<script type="math/tex; mode=display">O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=1}^{M} \frac{P(D_t = 1|Q,R=1)}{P(D_t = 1|Q,R=0)} \cdot \prod_{t:D_t=0}^{M} \frac{P(D_t = 0|Q,R=1)}{P(D_t = 0|Q,R=0)}\quad(4)</script>

<blockquote>
  <p>將document term分為出現或是不出現,(3)→(4)</p>
</blockquote>

<script type="math/tex; mode=display">p_t = P(D_t = 1|Q,R=1)</script>

<script type="math/tex; mode=display">u_t = P(D_t = 1|Q,R=0)</script>

<blockquote>
  <p><script type="math/tex">p_t</script> 表示term出現在document且和query相關的機率<br />
<script type="math/tex">u_t</script> 表示term出現在doucment且和query不相關的機率</p>
</blockquote>

<script type="math/tex; mode=display">O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t}{u_t} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \quad(5)</script>

<blockquote>
  <p>假設<script type="math/tex">Q_t = 0 \; then \; p_t = u_t</script>(假設可以做改變)  <br />
意思是沒出現在query的term就不用考慮,只考慮<script type="math/tex">Q_t = 1</script>  <br />
左邊連乘積表示 query term found in document<br />
右邊連乘積表示query term not found in document</p>
</blockquote>

<script type="math/tex; mode=display">O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \cdot \frac{1-p_t}{1-u_t} \quad(6)</script>

<blockquote>
  <p>右邊連乘積乘上 <script type="math/tex">\frac{1-p_t}{1-u_t}</script>, 所以所邊必須要除<script type="math/tex">\frac{1-p_t}{1-u_t}</script>才會相等</p>
</blockquote>

<script type="math/tex; mode=display">O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:Q_t=1} \frac{1-p_t}{1-u_t} \quad(7)</script>

<blockquote>
  <p>右邊連乘積是query not found in document<br />
概念大概是將query found in document也計算進去,<br />
不管有沒有出現在document都乘<br />
整理後右邊連乘積的範圍就會和document無關 <br />
在對document ranking時就視為常數</p>
</blockquote>

<script type="math/tex; mode=display">RSV_d = log \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} = \sum_{t:D_t=Q_t=1} log \frac{p_t(1-u_t)}{u_t(1-p_t)} \quad(8)</script>

<blockquote>
  <p>取log後就得到Retrieval Status Value(RSV),<br />
log是monotonic function不會改變ranking順序</p>
</blockquote>

<p><strong>RSJ Model:No Relevance Info</strong></p>

<script type="math/tex; mode=display">log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{p_t(1-u_t)}{u_t(1-p_t)}</script>

<p>如果沒有給relevance judgements,</p>
<ul>
  <li>assume <script type="math/tex">p_t</script> to be a constant</li>
  <li>Estimate <script type="math/tex">u_t</script> by assume all documents to be non-relevant</li>
</ul>

<p>1979 Croft&amp;Harper</p>

<script type="math/tex; mode=display">log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{N - n_t + 0.5}{n_t + 0.5}</script>

<blockquote>
  <p>N: number of documents in collection<br />
<script type="math/tex">n_t</script> : number of documents in which term <script type="math/tex">D_t</script> occurs</p>
</blockquote>

<script type="math/tex; mode=display">\sum log( \frac{總文章數 - 某個字出現在文章次數 + 0.5}{某個字出現在文章次數 + 0.5})</script>

<blockquote>
  <p>只看在document中和query相關的字,並加總每個字算出來的值</p>
</blockquote>

<p><strong>RSJ Model: with Relevance Info</strong></p>
<ul>
  <li>Maximum Lieklihood Estimate(MLE)</li>
  <li>Maximum A Posterior(MAP)</li>
</ul>

<blockquote>
  <p>RSJ model的performance還遠比不上vector space model</p>
</blockquote>

<p><strong>Improving RSJ</strong></p>
<ul>
  <li>adding TF</li>
  <li>adding Doc.length</li>
  <li>query TF</li>
</ul>

<p>改善後的最終公式稱作<strong>BM25</strong></p>

<hr />

<!-- 20170416 -->
<h1 id="ch12-language-models-for-information-retrieval">CH12 Language models for information retrieval</h1>

<p><strong>unigram language model</strong></p>
<blockquote>
  <p>每個word只有單一的狀態,可以建立一個table放每個word對應到的機率</p>
</blockquote>

<p>一個string出現的機率就是每個word的機率連乘積 <br />
Language model應用：語音系統的語言校正</p>

<p>Language model屬於query generation process  <br />
每一篇document視為一個language model<br />
ranking的計算是根據P(Q|D)</p>

<p><strong>計算P(Q|D)</strong></p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial model</a> <br />
  <img src="/img/websearching/MultinomialDistribution.png" alt="multinomial Distribustion" /></li>
</ul>

<script type="math/tex; mode=display">P(q|M_d) = P((t_1,...,t_{|q|})|M_d) = \prod_{1 \leq k \leq |q|} P(t_k|M_d) = \prod_{distinct\;term\;t\;in\;q}P(t|M_d)^{tf_{t,q}}</script>

<blockquote>
  <p>|q|: length of query<br />
<script type="math/tex">t_k</script> : query的第k個位置的token<br />
<script type="math/tex">tf_{t,q}</script> : term frequency of t in q</p>
</blockquote>

<p><strong>估計參數</strong></p>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximun Likelihood Estimation(MLE)</a>
    <blockquote>
      <script type="math/tex; mode=display">\hat{P}(t|M_d) = \frac{tf_{t,d}}{|d|}</script>
    </blockquote>
  </li>
</ul>

<div class="language-text highlighter-rouge"><pre class="highlight"><code>hat符號表示估計值的意思  
</code></pre>
</div>

<p><strong>smooth the estimates to avoid zeros</strong></p>
<blockquote>
  <p>避免0產生,相乘後結果很差</p>
</blockquote>

<p>smooth方法</p>
<ol>
  <li>Mixture model<br />
 <script type="math/tex">P(t|d)=\lambda P(t|M_d) + (1-\lambda )P(t|M_c)</script>
    <blockquote>
      <p><script type="math/tex">M_c</script> : the collection model<br />
 <script type="math/tex">\lambda</script> 的設定好壞會影響效能</p>
    </blockquote>

    <p>e.g.<br />
 Collection = {<script type="math/tex">d_1,d_2</script>}<br />
 <script type="math/tex">d_1</script>: Jack wants to play game<br />
 <script type="math/tex">d_2</script>: Tom is cat<br />
 query q: Tom game<br />
 <script type="math/tex">\lambda = \frac{1}{2}</script></p>

    <p>P(d|<script type="math/tex">d_1</script>) = [(0/5 + 1/8)/2]<script type="math/tex">\cdot</script>[(1/5 + 1/8)/2] <script type="math/tex">\approx</script> 0.0101 <br />
 P(d|<script type="math/tex">d_2</script>) = [(1/3 + 1/8)/2]<script type="math/tex">\cdot</script>[(0/3 + 1/8)/2] <script type="math/tex">\approx</script> 0.0143<br />
 rank <script type="math/tex">d_2 > d_1</script></p>
  </li>
  <li><a href="https://www.youtube.com/watch?v=gCI-ZC7irbY">Laplace smoothing</a>
<!-- 20170416  --></li>
</ol>

<!-- 20170427 -->
<p><strong>Text Generation with Unigram LM</strong></p>
<ul>
  <li>sampling  <br />
  由一個特定主題的model,裡面會有各個字出現的機率(每一個model會有一個distribution,機率分佈),取出一些機率較高的字可以形成document</li>
  <li>estimation<br />
  拿到一個document,預估出model</li>
</ul>

<!-- 20170427 -->

<!-- class -->
<hr />

<h1 id="ch13-text-classification-and-naive-bayes">CH13 Text Classification and Naive Bayes</h1>
<ul>
  <li>Text Classification</li>
  <li>Naive Bayes</li>
  <li>Naive Bayes Theory</li>
  <li>Evaluation of Text Classification</li>
</ul>

<h4 id="text-classification">Text Classification</h4>
<p><strong>standard supervised</strong></p>
<ol>
  <li>Pre-define categories and lebel document</li>
  <li>classify new documents</li>
</ol>

<p>分類的方法</p>
<ol>
  <li>人工判斷
 準確但成本高</li>
  <li>Rule-based
 很多if/else的rule,看到王建民就分類到體育新聞  <br />
 e.g. google Alert</li>
  <li>Statistical/Probabilistic
    <ul>
      <li>Instance-based classifiers<br />
  e.g. kNN</li>
      <li>Discriminative classifiers<br />
  學習出分隔的形式(一條線,一棵樹)<br />
  資料少容易overfit<br />
  e.g. Decision tree,Neural Network</li>
      <li>Generative classifier<br />
  利用大量資料學習出分佈模型(mean,varience)<br />
  e.g Naive Bayes</li>
    </ul>
  </li>
</ol>

<p><strong>K-Nearest Neighbor Classifier(KNN)</strong><br />
Keep all train data<br />
優點:不需要training(每一個點都記錄下來)<br />
缺點:不能做大量資料</p>

<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<p>arg max_c 找到一個c(類別)使P(c|d)最大
避免under flow -&gt; 取log連乘變成連加</p>

<p>機率會有零產生-&gt;避免這種情形全部機率做加1</p>

<p>估計事前機率 是這個類別的機率和不是這個類別的機率
估計完這些係數training就結束</p>

<p>P(c|d) 給document判斷是哪個類別
P(c|d) = P(c)P(d|c)/P(d)  分母不考慮,和分類無關
且分子越大越好
P(d|c)可以拆成多個Term的連乘積
且假設每個字之間獨立</p>

<p>Feture Selection</p>
<ul>
  <li>Reduces training time
挑特定的字訓練模型</li>
</ul>

<p>Two idea</p>
<ul>
  <li>Mutual information
  計算字的交互作用,每一個字會有一個值,找gap最大的切開,拿比較相關的字做訓練</li>
  <li>CHI-Square statistic
  用機率方法計算,算出機率高的就拿去做訓練資料</li>
</ul>

<p>Evaluations
    測試資料和training data不能有overlapping</p>

<!-- class -->

<hr />

<!---------------------------- 20170517 -------------------------------------------->
<h1 id="ch14-vector-space-classification">CH14 Vector Space Classification</h1>
<ul>
  <li><a href="#ch14_1">Rocchio classification</a></li>
  <li><a href="#ch14_2">kNN classification</a></li>
  <li><a href="#ch14_3">linear classification</a></li>
  <li><a href="#ch14_4">Bias-Variance Tradeoff</a></li>
</ul>

<h4 id="vector-space-classification-基本假設">Vector Space Classification 基本假設</h4>
<ol>
  <li>資料如果同類會形成一個連續的空間</li>
  <li>資料如果不同類別,所形成的空間不會有overlap</li>
</ol>

<h2 id="ch14_1">Rocchio classification(Nearest centroid classifier)</h2>
<p>將資料表示成向量形式,<br />
並將各類別的資料計算出重心(所有向量加總取平均),<br />
利用distance計算相似度<br />
計算的成本很低,但效果不太好(比naive bayes差)</p>

<p>主要是沒有處理：</p>
<ul>
  <li>nonconvex</li>
  <li>multimodal classes</li>
</ul>

<p>應用：</p>
<ul>
  <li>1970年SMART搜索系統中,應用在relevance feedback</li>
</ul>

<h2 id="ch14_2">k Nearest Neighbors(kNN) classification</h2>
<blockquote>
  <p>要分類的點根據最近的k個鄰居做投票決定</p>
</blockquote>

<p>k = 1 過於sensitive,太容易由某一類變成另外一類<br />
k太大結果過於模糊<br />
k通常選擇奇數(3,5,7),通常使用heuristic來決定k值</p>

<p>example:
k = 3
<img src="/img/websearching/kNN3.png" alt="kNN = 3" /></p>

<p>計算相似度:</p>
<ul>
  <li>Euclidean distance</li>
  <li>Hamming distance (binary instance)</li>
  <li>cosine similarity of tfidf</li>
</ul>

<p>特色:</p>
<ul>
  <li>沒有任何學習</li>
  <li>如果資料集很大,準確率非常高,資料集小可能就不會很準</li>
  <li>大致上準確率會較Naive Bayes和Rocchio高</li>
  <li>Simple, expensive at test time, high variance, non-linear</li>
</ul>

<!---------------------------- 20170517 -------------------------------------------->
<!---------------------------- 20170518 -------------------------------------------->

<h2 id="ch14_3">Linear classification</h2>
<blockquote>
  <p>想法是將所有的資料分為兩類,用一些公式計算出來的值大於一個門檻值分為一類,小於門檻值分到另一類</p>
</blockquote>

<script type="math/tex; mode=display">\sum{W_iX_i} > \theta, \quad \theta (threshold)</script>

<p>線性分類器代表一個分界面,在一維代表一個點、二維代表一條線、三維代表一個平面</p>

<p>找到分界面的演算法分為兩類</p>
<ol>
  <li>Simple learning algorithms<br />
 透過學習資料,學出分界面的參數
    <ul>
      <li>Naive Bayes,Rocchio,kNN</li>
    </ul>
  </li>
  <li>Iterative algorithms
    <ul>
      <li>Support vector machines,<a href="https://www.cs.utexas.edu/~teammco/misc/perceptron/">Perceptron</a></li>
    </ul>
  </li>
</ol>

<p>Linear Classifier算是多個方法的集大成<br />
e.g. Naive Bayes,Perceptron,Rocchio,Logistic regression,Support vector machines(with linear kernel),Linear regression with threshold</p>

<h2 id="ch14_4">Bias-Variance Tradeoff</h2>
<blockquote>
  <p>一種量測分類方法的指標</p>
</blockquote>

<p><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-variance wiki</a></p>

<ul>
  <li>Bias <br />
  分類的結果和真實結果差距多少<br />
  差距越大Bisa越高</li>
  <li>Variance<br />
  比較每次猜出來的結果差異會不會很大</li>
</ul>

<p>理想情形是low Bias, low Variance</p>

<script type="math/tex; mode=display">Err(x) = Bias^2 + Variance + Irreducible Error</script>

<!---------------------------- 20170518 -------------------------------------------->

<!---------------------------- 20170526 -------------------------------------------->
<h1 id="ch15-support-vector-machines-and-machine-learning-on-documents">Ch15 Support Vector Machines and Machine Learning on Documents</h1>

<p>在linear classification中,分界線有無限多條,SVM可以在這些線中找到一條最佳的線,而這條線只會由少數的點來形成。這些點是由分割上最困難的點所成的集合,集合稱作support vector</p>

<p>要怎麼找到最佳的分割界面(hyperplane)<br />
SVM是從所有可能的分割界面中找到geometric margin最大的作為分割界面</p>

<p>Geometric Margin</p>
<blockquote>
  <p>訓練出來的線,兩端碰到最近的support vector中的點所為成的區域</p>
</blockquote>

<p>目標是找到最大的$\rho$
<img src="/img/websearching/svm01.png" alt="svm01" /></p>

<ul>
  <li>w: hyperplane normal vector</li>
  <li>$x_i$: data point i</li>
  <li>$y_i$: class of data point (+1 or -1)</li>
  <li>classifier: f($x_i$) = sign($w^Tx_i$ + b)    //b: bias</li>
  <li>Define functional margin of $x_i$ is $y_i(w^Tx_i+b)$</li>
</ul>

<p>如果分到1類別帶入分類器會$\geq$1 <br />
分到-1類別帶入分類器會$\leq$-1</p>

<p>$w^Tx_i + b \geq 1 \quad if\; y_i = 1$<br />
$w^Tx_i + b \leq -1 \quad if\; y_i = -1$</p>

<p><img src="/img/websearching/svm02.png" alt="svm02" /></p>

<p>$\rho = \frac{2}{\left | w \right |}$</p>

<p><a href="https://en.wikipedia.org/wiki/Quadratic_programming">quadratic optimization problem</a></p>

<blockquote>
  <p>找到w和b使$\rho = \frac{2}{\left | w \right |}$最大<br />
$w^Tx_i + b \geq 1 \quad if\; y_i = 1$<br />
$w^Tx_i + b \leq -1 \quad if\; y_i = -1$</p>
</blockquote>

<p><strong>較好的表示方法</strong></p>

<blockquote>
  <p>要讓$\rho$最大,最小化$\left | w \right |$<br />
找到w和b使$\phi(w) = \frac{1}{2} \left | w \right |$最小  <br />
$y_i(w^Tx_i + b) \geq 1$</p>
</blockquote>

<p>希望$y_i和(w^Tx_i$ + b)同號 <br />
也就是希望分類結果和帶入分類器結果是同號</p>

<!---------------------------- 20170526 -------------------------------------------->

<!---------------------------- 20170601 -------------------------------------------->
<p>dual problem</p>
<blockquote>
  <p>每一個線性規劃的問題(primary problem)都有一個對映的線性規劃問題(dual problem),解決dual problem就可以得到原來問題的解</p>
</blockquote>

<p>解決上面問題的dual problem<br />
<strong>Lagrange multiplier</strong></p>
<blockquote>
  <p>找到$\alpha_1,…,\alpha_N$使$\sum{\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\vec{x}_i^T\vec{x}_j}$最大</p>
  <ul>
    <li>$\sum{\alpha_iy_i} = 0$</li>
    <li>$\alpha_i \geq 0 \quad for \; all \; 1 \leq i \leq N$</li>
  </ul>
</blockquote>

<p><strong>solution</strong></p>
<blockquote>
  <p>$\vec{w}=\sum\alpha_iy_i\vec{x}_i$  <br />
$b = y_k-\vec{w}^T\vec{x}_k\quad for\; any\; \vec{x}_k \; such\; that\; \alpha_k \neq 0$</p>
</blockquote>

<p>大多數的點$\alpha_i = 0$代表不重要的點,而非零的$\alpha$代表為support vector</p>

<p>分類的function</p>
<blockquote>
  <p>$f(\vec{x}) = sign(\sum\alpha_iy_i\vec{x_i}^T\vec{x} + b)$
<!---------------------------- 20170601 --------------------------------------------></p>
</blockquote>

<!---------------------------- class -------------------------------------------->
<p>soft margin classification
允許某些點可以分錯
$\phi$(w) 1/2||w|| + C$\sum$ slack
C 越小越可以容錯
因為希望$\phi$(w) 出來的值可以越小越好</p>

<h4 id="non-linear-svms">Non-linear SVMs</h4>
<p>用一條線分不開的情形,
svm提供kernel function的概念,將點map到更高維的空間中,就有機會能夠分割</p>

<p>kernel function
$K(x_i,x_j)=x_i^Tx_j$</p>

<p>透過kernel function可以知道高維空間的內積值</p>

<h4 id="common-kernels">Common kernels</h4>
<ul>
  <li>Linear</li>
  <li>Polynomial $$</li>
  <li>Radial basis function(infinite dimensional space)
<!---------------------------- class -------------------------------------------->
<!---------------------------- class -------------------------------------------->
    <h1 id="ch16-flat-clustering">Ch16 Flat clustering</h1>
  </li>
</ul>

<h4 id="classification-分類-vs-clustering-分群">Classification 分類 vs. Clustering 分群</h4>

<ul>
  <li>supervised learning</li>
  <li>Unsupervised learning
  資料沒有label,沒有正確答案</li>
  <li>reinforcement learning</li>
</ul>

<h4 id="flat-vs-hierarchical-clustering">Flat vs. Hierarchical clustering</h4>

<ul>
  <li>Flat
    <ul>
      <li>Hard clustering
  一個東西就只能分到一類<br />
  e.g. K-means</li>
      <li>Soft clustering<br />
  比較偏向使用機率分群</li>
    </ul>
  </li>
  <li>Hiearchical clustering<br />
  希望分群完後是一種階層架構</li>
</ul>

<h4 id="k-means">K-means</h4>

<ul>
  <li>Each cluster in K-means is defined by a centroid</li>
</ul>

<h4 id="method">Method</h4>
<ul>
  <li>reassignment of vector</li>
  <li>recomputation of centroids</li>
</ul>

<ol>
  <li>init
 隨機給重心點</li>
  <li>reassign</li>
  <li>recomputation of centroids</li>
</ol>

<p>什麼時候收斂？
RSS(residual sum of squares)
每次的centroid差值小於某個值就停止</p>

<p>缺點是
initial的點很糟結果會不好和k怎麼決定</p>

<p>time complexity
O(IKNM)</p>

<h4 id="evaluation">Evaluation</h4>
<ul>
  <li>Internal criteria
  RSS的評估</li>
  <li>External criteria
  goal base,必須要知道正確答案
    <ul>
      <li>Purity
  必須要有正確答案,並和分出的結果做比較</li>
    </ul>
  </li>
</ul>

<h4 id="怎麼決定k值">怎麼決定k值</h4>
<ol>
  <li>外部資訊</li>
  <li>從1開始慢慢加上去,並且計算criteria是否有增加</li>
</ol>

<h1 id="ch21-link-analysis">Ch21 Link Analysis</h1>
<p>Page Rank
<!---------------------------- class --------------------------------------------></p>

<!---------------------------- class -------------------------------------------->
<p>Ch21 Link Analysis</p>

<ul>
  <li>Anchor Text</li>
  <li>Citation Analysis</li>
  <li>PageRank</li>
  <li>HITS: Hub &amp; Authorties</li>
</ul>

<h4 id="citation-analysis">Citation Analysis</h4>
<p>Citation frequency can be used to meature impact</p>

<h4 id="pagerank">PageRank</h4>
<p>model behind Random walk
在所有網頁上隨機走動,且根據link連到其他網頁
到穩定狀態(steady state)後,計算停留在每個page的機率
PageRank = long-term visit rate</p>

<p>將網頁連結的關係使用矩陣來存
Dead end,如果有一些網頁指到同一個網頁如果之後就跳不出去會有not well define的問題
teleporting解決這個問題,給一個很低的機率讓所有0的網頁都有機會跳到別的網頁,
類似smoothing,
每個點會有一個機率,且乘上矩陣一次會走一步
那一直走下去會到收斂狀態($\pi$)</p>

<p>P is transifer matrix
$\pi = \pi P$</p>

<p>Power method -&gt; 一直乘P,直到向量收斂</p>

<h4 id="hits--authorities">HITS &amp; Authorities</h4>
<ol>
  <li>Hub 
 像是入口網站,有很多連出去的連結(out link)</li>
  <li>Authorities 
 網站內的資料是屬於比較官方的網站
 資料屬於比較權威(就是多的link指向的網頁)</li>
</ol>

<p>不停的迭代
$h = Aa$<br />
$a = A^Th$</p>

<p>A 代表相鄰矩陣
a 的初始值($a_0$)在第一輪都設定為一樣</p>

<!---------------------------- class -------------------------------------------->

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/%E5%90%83%E4%BA%86%E9%82%A3%E9%9A%BB%E9%9D%92%E8%9B%99/">
            吃了那隻青蛙
            <small>25 Feb 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/readinglist/">
            讀書清單
            <small>25 Feb 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2017/dataStructure/">
            Data Structure
            <small>27 Mar 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>

  <hr>
  <!-- DISQUS comments -->
  
<div id="disqus_thread"></div>
<script>

var disqus_shortname = 'Note'; // required: replace example with your forum shortname
var disqus_identifier = '/2017/webSearching/';
var disqus_url = 'https://moved0311.github.com/2017/webSearching/';

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-moved0311-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</div>


    </div>
    
  </body>
</html>

