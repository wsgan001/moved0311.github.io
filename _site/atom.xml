<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Note</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2017-04-27T13:10:33+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Taiyi</name>
   <email>moved0311@gmail.com</email>
 </author>

 
 <entry>
   <title>Data Structure</title>
   <link href="http://localhost:4000/2017/dataStructure/"/>
   <updated>2017-03-27T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/dataStructure</id>
   <content type="html">&lt;h2 id=&quot;linklist&quot;&gt;LinkList&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;linklist reverse&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/img/dataStructure/linkListReverse.png&quot; alt=&quot;linkListReverse&quot; /&gt;
&lt;!--more--&gt;
    &lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;linkListReverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;   
              &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;null&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;  
             &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
        
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;before reverse:&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverseHead&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;After reverse:&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverseHead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>python</title>
   <link href="http://localhost:4000/2017/python/"/>
   <updated>2017-03-08T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/python</id>
   <content type="html">&lt;h3 id=&quot;time&quot;&gt;Time&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# your code here.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;time : &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; seconds.&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;input&quot;&gt;input&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;raw_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;What is your name?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;!--more--&gt;
&lt;h3 id=&quot;if&quot;&gt;if&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# False&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yes&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;hello world&quot;&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;'hello'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;rain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gohome&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rain&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;tuple&quot;&gt;tuple&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;123&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;456&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hello'&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#tuple&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# a = 123, b = 456, c = 'hello'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;dictionary&quot;&gt;dictionary&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Apple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Cat&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Apple&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 1&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Banana&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 100 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;library&quot;&gt;library&lt;/h3&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;division&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 1/2 = 0.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;reference&quot;&gt;reference:&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://seanlin.logdown.com/posts/206973-python-idioms-1&quot;&gt;Python 慣用語&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>WebSearching</title>
   <link href="http://localhost:4000/2017/webSearching/"/>
   <updated>2017-03-07T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/webSearching</id>
   <content type="html">&lt;h2 id=&quot;ch8-evaluation-in-information-retrival&quot;&gt;CH8 Evaluation in information retrival&lt;/h2&gt;
&lt;p&gt;評量search engine好壞&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;搜到的index&lt;/li&gt;
  &lt;li&gt;搜尋速度&lt;/li&gt;
  &lt;li&gt;二氧化碳排放量&lt;/li&gt;
  &lt;li&gt;和搜尋相關程度&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;
&lt;p&gt;相關程度&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;benchmark資料集&lt;/li&gt;
  &lt;li&gt;benchmark queries(問句)&lt;/li&gt;
  &lt;li&gt;文章是否相關的標記(ground truths)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;queries和information need有落差
想找的東西,不會下key word&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision (P)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Precision = &lt;script type=&quot;math/tex&quot;&gt;\frac{檢索到相關物件的數量}{物件總數}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Recall (R)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Recall = &lt;script type=&quot;math/tex&quot;&gt;\frac{檢索到相關物件的數量}{相關物件總數}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Relevant&lt;/td&gt;
      &lt;td&gt;Nonrelevent&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Retrieved&lt;/td&gt;
      &lt;td&gt;true positive(tp)&lt;/td&gt;
      &lt;td&gt;false positive(fp)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Not retrieved&lt;/td&gt;
      &lt;td&gt;false negatives(fn)&lt;/td&gt;
      &lt;td&gt;true negatives(tn)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;true positive(tp) 機器判斷+且為真&lt;br /&gt;
false positive(fp)機器判斷+但是是假&lt;br /&gt;
false negatives(fn) 機器判斷-但判斷錯&lt;br /&gt;
true negatives(tn)  機器判斷-且判斷對&lt;/p&gt;

&lt;p&gt;P = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp}{tp+fp}\&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;R = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp}{tp+fn}\&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Accuracy vs Precision&lt;/strong&gt; &lt;br /&gt;
accuracy = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp+tn}{tp+fp+fn+tn}\&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;accuracy不適合用在information retrieval,
因為通常Nonrelevant會非常的大,tn項非常大除分母fn和tn都非常大結果會趨近於1,所以才用Precision和Recall作為依據&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;調和平均數(harmonic mean)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;H = &lt;script type=&quot;math/tex&quot;&gt;\frac{n}{\frac{1}{x_1}+\frac{1}{x_2}+..+\frac{1}{x_n}}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Precision/Recall Tradoff
使用調和平均數計算Precision和Recall的Tradoff&lt;br /&gt;
量測的數值稱做F measure,α和1-α分別為P和R的權重,一般是取α=0.5&lt;br /&gt;
(P和R重要程度相同)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;F = &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\alpha\frac{1}{P}+(1-\alpha)\frac{1}{R}} = \frac{(\beta^2+1)PR}{\beta^2P+R}\&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\ \beta^2=\frac{1-\alpha}{\alpha}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;ranked-evalution&quot;&gt;Ranked Evalution&lt;/h4&gt;
&lt;p&gt;P,R,F都是unordered(沒有等級)&lt;br /&gt;
一個query會有一個Precision/Recall圖    &lt;br /&gt;
使用內插法(interpolated)可以得到一張較平滑的P-R圖&lt;br /&gt;
(和機器學習ROC curve相似)&lt;br /&gt;
P-R curve的面積越大效能越佳(代表Precision掉越慢)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;內插法&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ p_{interp}= \max\limits_{r'\ge r}\ p(r{'})\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;r代表recall,
作法是從目前往後找最高的點向前填平,並重新畫P-R圖&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Average Precision(MAP)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ MAP(Q) = \frac{1}{|Q|}\sum^{|Q|}_{j=1}\frac{1}{m_j}\sum_{k=1}^{m_j}Precision(R_{jk})\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一個sum算query平均&lt;br /&gt;
第二個sum算precision平均&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision at k&lt;/strong&gt;&lt;br /&gt;
第k個搜索結果的Precision&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-Precision&lt;/strong&gt;&lt;br /&gt;
文件中總共有R篇相關文章,以R作為cut-off,計算Precision&lt;br /&gt;
e.g. 總共有100篇文章,其中10篇是相關的&lt;br /&gt;
且搜尋結果是:RNRNN RRNNN RNNRR ….&lt;br /&gt;
R=10(只看RNRNN RRNNN)計算Precision&lt;br /&gt;
R-Precision = 0.4&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Normalized Discounted Cumulative Gain(NDCG)&lt;/strong&gt;  &lt;br /&gt;
作者：Kalervo Jarvelin, Jaana Kekalainen(2002)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;用來衡量ranking quality&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;e.g.&lt;br /&gt;
G = &amp;lt;3,2,3,0,0,1,2,2,3,0,…&amp;gt; &lt;br /&gt;
G表示一個搜索的結果(3高度相關, 0沒關係)&lt;br /&gt;
步驟:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cumulative Gain(CG)
    &lt;blockquote&gt;

      &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\
 CG[i] = \left\{\begin{matrix}
 G[1], &amp;if\ i=1 \\ 
 CG[i-1]+G[i], &amp;otherwise 
 \end{matrix}\right.
 \ %]]&gt;&lt;/script&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;目前項+＝前一項(做成一個遞增的函數)&lt;br /&gt;
 CG’=&amp;lt;3,5,8,8,8,9,11,13,16,16,…&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Discounted Cumulative Gain(DCG)
    &lt;blockquote&gt;

      &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\
 DCG[i]=\left\{\begin{matrix}
 G[i], &amp; if\ i=1\\ 
 DCG[i-1]+G[i]/log_b\ i, &amp; otherwise
 \end{matrix}\right.
 \ %]]&gt;&lt;/script&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;DCG’=&amp;lt;3,5,6.89,6.89,6.89,7.28,7.99,8.66,9.61,9.61,…&amp;gt; if b=2&lt;br /&gt;
 i代表排名,對排名做懲罰(除log&lt;sub&gt;2&lt;/sub&gt; i),排名越後面懲罰越重&lt;br /&gt;
 代表如果搜尋的結果很差,和理想的排序分數會相差很多&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Normalized Discounted Cumulative Gain(NDCG)&lt;br /&gt;
 理想的搜索結果I=&amp;lt;3,3,3,2,2,2,1,1,1,1,0,0,0,…&amp;gt;(高度相關的排越前面) &lt;br /&gt;
 理想搜索結果DCGI=&amp;lt;3,6,7.89,8.89,9.75,10.52,10.88,11.21,…&amp;gt;&lt;br /&gt;
 nDCG&lt;sub&gt;n&lt;/sub&gt; = &lt;script type=&quot;math/tex&quot;&gt;\ \frac{DCG_{n}}{IDCG_{n}}(\frac{相關程度排序}{理想相關程度},做正規化)\&lt;/script&gt;&lt;br /&gt;
 NDCG=&amp;lt;1,0.83,0.87,0.77,0.70,0.69,0.73,0.77,…&amp;gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;benchmark 資料集&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cranfield&lt;/li&gt;
  &lt;li&gt;TREC(nist.gov)&lt;br /&gt;
 Ad-hoc 資料集(1992-1999)&lt;/li&gt;
  &lt;li&gt;GOV2
 2500萬篇文章&lt;/li&gt;
  &lt;li&gt;NICIR
 cross-language IR&lt;/li&gt;
  &lt;li&gt;Cross Language Evaluation&lt;/li&gt;
  &lt;li&gt;REUTERS&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;標記資料準則&lt;/strong&gt; &lt;br /&gt;
Kappa measure&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;標記資料是否一致的衡量標準,若標記不一致資料中就沒有truth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kappa計算公式&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \kappa = \frac{P(A)-P(E)}{1-P(E)}\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;搜尋結果的呈現 Result Summaries&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;搜尋結果呈現：10 blue link&lt;/li&gt;
  &lt;li&gt;搜尋結果下方文字說明分為Static和Dynamic
Static:固定抽前50個字
Dynamic:利用nlp技術,根據搜索關鍵字動態做變化&lt;/li&gt;
  &lt;li&gt;quicklinks&lt;br /&gt;
底下多的連結&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ch6-model&quot;&gt;Ch6 Model&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Vector Space Model&lt;/li&gt;
  &lt;li&gt;Probabilistic Information Retrieval&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Empirical IR&lt;/td&gt;
      &lt;td&gt;Model-based IR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;暴力法&lt;/td&gt;
      &lt;td&gt;有理論模型&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;heuristic&lt;/td&gt;
      &lt;td&gt;數學假設&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;難推廣到其他問題&lt;/td&gt;
      &lt;td&gt;容易推廣到其他問題&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;IR model歷史&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1960&lt;br /&gt;
  第一個機率模型&lt;/li&gt;
  &lt;li&gt;1970   &lt;br /&gt;
  vector space model(75)  &lt;br /&gt;
  classic probabilistic model(76)&lt;/li&gt;
  &lt;li&gt;1980&lt;br /&gt;
  non-class logic model(86)&lt;/li&gt;
  &lt;li&gt;1990&lt;br /&gt;
  TREC benchmark &lt;br /&gt;
  BM25/Okapi(94)&lt;br /&gt;
  google成立(96)  &lt;br /&gt;
  Language model(98)&lt;/li&gt;
  &lt;li&gt;2000&lt;br /&gt;
  Axiomatic model(04)&lt;br /&gt;
  Markov Random Field(05)   &lt;br /&gt;
  Learning to rank(05)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Vector space&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Vocabulary&lt;/td&gt;
      &lt;td&gt;V = { &lt;script type=&quot;math/tex&quot;&gt;w_1,w_2,w_3,...w_v&lt;/script&gt; }&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Query&lt;/td&gt;
      &lt;td&gt;q =&lt;script type=&quot;math/tex&quot;&gt;\{q_1,q_2,...,q_m\}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Document 文章&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;{d_i} = \{  w_1,w_2,...  \}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Collection文章集合&lt;/td&gt;
      &lt;td&gt;C = { &lt;script type=&quot;math/tex&quot;&gt;d_1,d_2,d_3,...&lt;/script&gt; }&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;R(q) query的集合&lt;/td&gt;
      &lt;td&gt;R(q) ⊂ C&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;目標是找到近似query的集合&lt;/strong&gt;&lt;br /&gt;
策略:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Document select&lt;br /&gt;
 挑文件如果是相關就收到集合&lt;br /&gt;
 absolute relevance(系統必須決定是相關還是不相關)&lt;/li&gt;
  &lt;li&gt;Document ranking&lt;br /&gt;
 query的結果&amp;gt;threshold 就收進去  &lt;br /&gt;
 relative relevance(不必是1或0,相近到一定程度就收進集合)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Probability Ranking Principle(PRP)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Robertson (1977)&lt;br /&gt;
相似度量測函數f滿足,&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;f(q,d_1) &gt; f(q,d_2)\quad iff\quad p(Rel|q,d_1) &gt; p(Rel|q,d_2)&lt;/script&gt;  &lt;br /&gt;
f()值越大表示有越大的機率越相似&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Relevance流派&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Similarity 相似度&lt;br /&gt;
   Vector space model(Salton et al,75)&lt;/li&gt;
  &lt;li&gt;Probability of relevance 機率模型
  Classical probaility Model(Robertson&amp;amp;Sparck Jones,76)&lt;br /&gt;
  Learning to Rank(Joachims,02, Berges et al,05)&lt;/li&gt;
  &lt;li&gt;Probability inference 機率推論&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Vector Space Model(VSM)&lt;/strong&gt;&lt;br /&gt;
將query和document表示成向量形式(similar representation)&lt;br /&gt;
假設Relevance(d,q) = similar(d,q)&lt;br /&gt;
利用cosine算相似度(1 ~ -1) &lt;br /&gt;
high dimension(index的維度通常在10萬左右) &lt;br /&gt;
good dimension -&amp;gt; orthogonal&lt;br /&gt;
(好的維度切割應該是維度間彼此獨立(orthogonal),&lt;br /&gt;
但是通常很困難,例如nccu後面接university的機率很高)&lt;br /&gt;
VSM優點: Empirically effective,直觀, 實作容易&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VectorSpace範例程式&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html&quot;&gt;Building a Vector Space Search Engine in Python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;大致步驟&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;將所有文章使用join()成為一個string包含所有文章內容&lt;/li&gt;
  &lt;li&gt;做string clean去除&lt;code class=&quot;highlighter-rouge&quot;&gt;.&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;,&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;多餘空白&lt;/code&gt;,並轉為小寫&lt;/li&gt;
  &lt;li&gt;將clean好的string利用空白切分成words array,丟到&lt;a href=&quot;https://tartarus.org/martin/PorterStemmer/&quot;&gt;Porter stem&lt;/a&gt;(去除字尾)
    &lt;blockquote&gt;
      &lt;p&gt;Porter Stemming Algorithm&lt;br /&gt;
    作者:Martin Porter(2006)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;刪除重複的word,使用set讓出現的word唯一&lt;/li&gt;
  &lt;li&gt;得到所有整理完的words,做成index(將每個word編號),類似字典的概念&lt;/li&gt;
  &lt;li&gt;將每篇文章分別建立自己的vector,並統計每個word出現的次數(term frequecy)&lt;/li&gt;
  &lt;li&gt;將輸入的query做成vector&lt;/li&gt;
  &lt;li&gt;利用兩個向量做cosine計算相關程度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;相似度計算&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cosine Similarity
    &lt;blockquote&gt;
      &lt;p&gt;cosine = &lt;script type=&quot;math/tex&quot;&gt;\frac{V_1 \cdot V_2}{\|V_1\|\|V_2\|}&lt;/script&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Jaccard Similarity
    &lt;blockquote&gt;
      &lt;p&gt;相似度 = &lt;script type=&quot;math/tex&quot;&gt;\frac{交集}{聯集}&lt;/script&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;TF-IDF Weighting&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TF(Term Frequency)&lt;br /&gt;
  word count,單純統計字數出現頻率&lt;/li&gt;
  &lt;li&gt;IDF(Inverse Document Frequency)(反向的TF)&lt;br /&gt;
  字的獨特性,如果某些字在很多篇文章出現次數都很高(例如:the,a,to,…) &lt;br /&gt;
  IDF值就會很低(沒有鑑別度)	&lt;br /&gt;
  IDF(t) = 1 + log(n/k)  (n:篇數,k:字出現次數)&lt;br /&gt;
  例如文章總數是1000(n=1000),所有文章都有出現cat(k=1000),&lt;br /&gt;
  IDF = 1 + log(1000/1000) = 1&lt;br /&gt;
  如果只有1篇文章有出現cat,&lt;br /&gt;
  IDF = 1 + log(1000/1) = 4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TF-IDF計算方法:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;weight(t,d) = TF(t,d) * IDF(t)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;TF-IDF範例程式&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/&quot;&gt;Tutorial: Finding Important Words in Text Using TF-IDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;ch11-probabilistic-information-retrieval&quot;&gt;Ch11 Probabilistic Information Retrieval&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Probability theory&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Joint probability &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A\cap B)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Conditional probability  &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A | B)&lt;/script&gt;&lt;br /&gt;
  probability of A given that event B occurred.&lt;/li&gt;
  &lt;li&gt;Chain rule &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A,B) = P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Partition rule&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(B) = P(A,B) + P(\bar{A},B)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;Chain rule example wiki&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有兩個甕第一個甕放1個黑球2個白球,第二個甕放1個黑球3個白球&lt;br /&gt;
事件A是選到第一個甕,事件B是選到白球&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(B|A)= \frac{2}{3}&lt;/script&gt; 在選到第一個甕的情況下拿到白球&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A,B)=P(B|A)P(A)=\frac{2}{3} \times \frac{1}{2}&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Bayes' Rule&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A|B) = \frac{P(B|A)P(A)}{P(B)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;from chain rule: &lt;script type=&quot;math/tex&quot;&gt;P(A|B)P(B) = P(B|A)P(A)&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P(A)    : 事前機率(prior probability)  
P(A|B)  : 事後機率(postior probability)  
P(B|A)  : likelihood   

The term likelihood is just a synonym of probability.   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Odds&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(A) = \frac{P(A)}{P(\bar{A})} = \frac{P(A)}{1-P(A)}&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;an event provide a kind of multiplier for how probabilities change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Probability of Relevance&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Random variables:
    &lt;ul&gt;
      &lt;li&gt;query Q&lt;/li&gt;
      &lt;li&gt;document D&lt;/li&gt;
      &lt;li&gt;relevance R ∈ {0,1} &lt;br /&gt;
(1:相關,0:不相關)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Goal: P(R=1|Q,D) to rank relevant&lt;br /&gt;
  利用query和document相似度的機率做排名&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Refining P(R=1|Q,D) Methods&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Conditional Models(Discriminative Models)
    &lt;ul&gt;
      &lt;li&gt;利用各種方法找出機率P = f(x)&lt;/li&gt;
      &lt;li&gt;利用資料訓練參數&lt;/li&gt;
      &lt;li&gt;利用model去排列未知的document&lt;br /&gt;
 e.g. Learning to rank,類神經網路,…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative Models
    &lt;ul&gt;
      &lt;li&gt;compute the odd of O(R=1|Q,D) using Bayes' rules&lt;br /&gt;
 先找出資料的分佈再做預測&lt;/li&gt;
      &lt;li&gt;How to define P(Q,D|R)
        &lt;ul&gt;
          &lt;li&gt;Document generation: P(Q,D|R)=P(D|Q,R)P(Q|R) &lt;br /&gt;
  query放到條件 (e.g RSJ model)&lt;/li&gt;
          &lt;li&gt;Query generation: P(Q,D|R)=P(Q|D,R)P(D|R) &lt;br /&gt;
  document放到條件 (e.g language model)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;rsj-model-binary-independence-model&quot;&gt;RSJ Model &lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_Independence_Model&quot;&gt;(Binary Independence Model)&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;利用&lt;strong&gt;Odd&lt;/strong&gt;值做ranking的依據:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(R|D,Q) = \frac{P(R=1|D,Q)}{P(R=0|D,Q)} = \frac{ \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q)} }{ \frac{P(D|Q,R=0)P(R=0|Q)}{P(D|Q)} } = \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q,R=0)P(R=0|Q)} \quad(1)&lt;/script&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{P(R=1|Q)}{P(R=0|Q)}&lt;/script&gt; 
對document ranking沒有影響,視為常數&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{P(D|Q,R=1)}{P(D|Q,R=0)} = \prod_{t=1}^{M} \frac{P(D_t|Q,R=1)}{P(D_t|Q,R=0)} \quad(2)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;將document拆成多個獨立的document term連乘積,且&lt;script type=&quot;math/tex&quot;&gt;D_t \in \{0,1\}&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t=1}^{M} \frac{P(D_t|R=1,Q)}{P(D_t|R=0,Q)} \quad(3)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;(2)代入(1)可以整理出(3)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=1}^{M} \frac{P(D_t = 1|Q,R=1)}{P(D_t = 1|Q,R=0)} \cdot \prod_{t:D_t=0}^{M} \frac{P(D_t = 0|Q,R=1)}{P(D_t = 0|Q,R=0)}\quad(4)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;將document term分為出現或是不出現,(3)→(4)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_t = P(D_t = 1|Q,R=1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_t = P(D_t = 1|Q,R=0)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p_t&lt;/script&gt; 表示term出現在document且和query相關的機率&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;u_t&lt;/script&gt; 表示term出現在doucment且和query不相關的機率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t}{u_t} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \quad(5)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;假設&lt;script type=&quot;math/tex&quot;&gt;Q_t = 0 \; then \; p_t = u_t&lt;/script&gt;(假設可以做改變)  &lt;br /&gt;
意思是沒出現在query的term就不用考慮,只考慮&lt;script type=&quot;math/tex&quot;&gt;Q_t = 1&lt;/script&gt;  &lt;br /&gt;
左邊連乘積表示 query term found in document&lt;br /&gt;
右邊連乘積表示query term not found in document&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \cdot \frac{1-p_t}{1-u_t} \quad(6)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;右邊連乘積乘上 &lt;script type=&quot;math/tex&quot;&gt;\frac{1-p_t}{1-u_t}&lt;/script&gt;, 所以所邊必須要除&lt;script type=&quot;math/tex&quot;&gt;\frac{1-p_t}{1-u_t}&lt;/script&gt;才會相等&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:Q_t=1} \frac{1-p_t}{1-u_t} \quad(7)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;右邊連乘積是query not found in document&lt;br /&gt;
概念大概是將query found in document也計算進去,&lt;br /&gt;
不管有沒有出現在document都乘&lt;br /&gt;
整理後右邊連乘積的範圍就會和document無關 &lt;br /&gt;
在對document ranking時就視為常數&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;RSV_d = log \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} = \sum_{t:D_t=Q_t=1} log \frac{p_t(1-u_t)}{u_t(1-p_t)} \quad(8)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;取log後就得到Retrieval Status Value(RSV),&lt;br /&gt;
log是monotonic function不會改變ranking順序&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;RSJ Model:No Relevance Info&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{p_t(1-u_t)}{u_t(1-p_t)}&lt;/script&gt;

&lt;p&gt;如果沒有給relevance judgements,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;assume &lt;script type=&quot;math/tex&quot;&gt;p_t&lt;/script&gt; to be a constant&lt;/li&gt;
  &lt;li&gt;Estimate &lt;script type=&quot;math/tex&quot;&gt;u_t&lt;/script&gt; by assume all documents to be non-relevant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1979 Croft&amp;amp;Harper&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{N - n_t + 0.5}{n_t + 0.5}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;N: number of documents in collection&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;n_t&lt;/script&gt; : number of documents in which term &lt;script type=&quot;math/tex&quot;&gt;D_t&lt;/script&gt; occurs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum log( \frac{總文章數 - 某個字出現在文章次數 + 0.5}{某個字出現在文章次數 + 0.5})&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;只看在document中和query相關的字,並加總每個字算出來的值&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;RSJ Model: with Relevance Info&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Maximum Lieklihood Estimate(MLE)&lt;/li&gt;
  &lt;li&gt;Maximum A Posterior(MAP)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RSJ model的performance還遠比不上vector space model&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Improving RSJ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;adding TF&lt;/li&gt;
  &lt;li&gt;adding Doc.length&lt;/li&gt;
  &lt;li&gt;query TF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改善後的最終公式稱作&lt;strong&gt;BM25&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;!-- 20170416 --&gt;
&lt;h2 id=&quot;ch12-language-models-for-informatio-retrieval&quot;&gt;CH12 Language models for informatio retrieval&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;unigram language model&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;每個word只有單一的狀態,可以建立一個table放每個word對應到的機率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一個string出現的機率就是每個word的機率連乘積 &lt;br /&gt;
Language model應用：語音系統的語言校正&lt;/p&gt;

&lt;p&gt;Language model屬於query generation process  &lt;br /&gt;
每一篇document視為一個language model&lt;br /&gt;
ranking的計算是根據P(Q|D)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;計算P(Q|D)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;Multinomial model&lt;/a&gt; &lt;br /&gt;
  &lt;img src=&quot;/img/websearching/MultinomialDistribution.png&quot; alt=&quot;multinomial Distribustion&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(q|M_d) = P((t_1,...,t_{|q|})|M_d) = \prod_{1 \leq k \leq |q|} P(t_k|M_d) = \prod_{distinct\;term\;t\;in\;q}P(t|M_d)^{tf_{t,q}}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;|q|: length of query&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; : query的第k個位置的token&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;tf_{t,q}&lt;/script&gt; : term frequency of t in q&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;估計參數&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Maximun Likelihood Estimation(MLE)&lt;/a&gt;
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{P}(t|M_d) = \frac{tf_{t,d}}{|d|}&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hat符號表示估計值的意思  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;smooth the estimates to avoid zeros&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;避免0產生,相乘後結果很差&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;smooth方法&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Mixture model&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;P(t|d)=\lambda P(t|M_d) + (1-\lambda )P(t|M_c)&lt;/script&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;M_c&lt;/script&gt; : the collection model&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; 的設定好壞會影響效能&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;e.g.&lt;br /&gt;
 Collection = {&lt;script type=&quot;math/tex&quot;&gt;d_1,d_2&lt;/script&gt;}&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt;: Jack wants to play game&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt;: Tom is cat&lt;br /&gt;
 query q: Tom game&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\lambda = \frac{1}{2}&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;P(d|&lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt;) = [(0/5 + 1/8)/2]&lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;[(1/5 + 1/8)/2] &lt;script type=&quot;math/tex&quot;&gt;\approx&lt;/script&gt; 0.0101 &lt;br /&gt;
 P(d|&lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt;) = [(1/3 + 1/8)/2]&lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;[(0/3 + 1/8)/2] &lt;script type=&quot;math/tex&quot;&gt;\approx&lt;/script&gt; 0.0143&lt;br /&gt;
 rank &lt;script type=&quot;math/tex&quot;&gt;d_2 &gt; d_1&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 20170416  --&gt;

&lt;!-- 20170427 --&gt;
&lt;p&gt;&lt;strong&gt;Text Generation with Unigram LM&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sampling  &lt;br /&gt;
  由一個特定主題的model,裡面會有各個字出現的機率(每一個model會有一個distribution,機率分佈),取出一些機率較高的字可以形成document&lt;/li&gt;
  &lt;li&gt;estimation&lt;br /&gt;
  拿到一個document,預估出model&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- 20170427 --&gt;

&lt;!-- class --&gt;
&lt;p&gt;MLE -&amp;gt; smoothing(去除機率為0的值)用copers這個smooth方法&lt;/p&gt;

&lt;p&gt;smoothing方法&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Jelinek-Mercer&lt;/li&gt;
  &lt;li&gt;Dirichlet prior&lt;/li&gt;
  &lt;li&gt;Absolute discontuning
&lt;!-- class --&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>SocialCloudComputing</title>
   <link href="http://localhost:4000/2017/cloudComputing/"/>
   <updated>2017-02-25T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/cloudComputing</id>
   <content type="html">&lt;ul&gt;
  &lt;li&gt;Centrality Analysis&lt;/li&gt;
  &lt;li&gt;Community Detection&lt;/li&gt;
  &lt;li&gt;Link Prediction&lt;/li&gt;
  &lt;li&gt;Label Prediction&lt;/li&gt;
  &lt;li&gt;Influence maximization&lt;/li&gt;
  &lt;li&gt;Outbreak Detection&lt;br /&gt;
消息擴散路徑&lt;/li&gt;
  &lt;li&gt;Role/Postion Analysis&lt;/li&gt;
  &lt;li&gt;Social Relation Extraction&lt;/li&gt;
  &lt;li&gt;Cloud Computing&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;journals&quot;&gt;Journals&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nature&lt;/li&gt;
  &lt;li&gt;Science&lt;/li&gt;
  &lt;li&gt;Physical Review&lt;/li&gt;
  &lt;li&gt;Social Networks&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Knowledge Discovery from Data (TKDD)&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Intelligent Systems and Technology(TIST)&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Social Computing(TSC)&lt;/li&gt;
  &lt;li&gt;IEEE Transactions on Knowledge and Data Engineering(TKDE)&lt;/li&gt;
  &lt;li&gt;IEEE Transactions on Computational Social System&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;social-networks&quot;&gt;Social Networks&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sociocentric&lt;/td&gt;
      &lt;td&gt;Egocentric&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;根據整群分析&lt;/td&gt;
      &lt;td&gt;根據個人分析,向外延伸&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;information Network&lt;br /&gt;
paper reference&lt;br /&gt;
web hyperlink&lt;br /&gt;
Language&lt;/li&gt;
  &lt;li&gt;Social Network&lt;br /&gt;
FB好友關係&lt;/li&gt;
  &lt;li&gt;Technology Network&lt;br /&gt;
電力系統(Power grid)&lt;/li&gt;
  &lt;li&gt;Biologycal Network&lt;br /&gt;
蛋白質互動關係,食物鏈&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;為什麼要分這麼多類Network?&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;因為要分析的點不同,可能在information Network中很重要的,卻在Social Network可能不是那麼重要&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;network-properties&quot;&gt;Network Properties&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;small-world effect
 六度分離理論&lt;br /&gt;
 靠點和點距離關係分析&lt;/li&gt;
  &lt;li&gt;Transitivity
 朋友的朋友很可能也是你朋友&lt;br /&gt;
 &lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%9B%86%E8%81%9A%E7%B3%BB%E6%95%B0&quot;&gt;Clustering Coeffieient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Degree distribution
 Real world network : Power law
    &lt;blockquote&gt;
      &lt;p&gt;P&lt;sub&gt;k&lt;/sub&gt; = CK&lt;sup&gt;-α&lt;/sup&gt;&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Heavy-tailed degree distribution&lt;br /&gt;
 大量很低的數量,集合起來還是很驚人&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Network resilience
 如果拿掉一些點/邊,連通性會有什麼變化？&lt;br /&gt;
 (e.g.有些人掛了,離職)&lt;br /&gt;
 連接path的長度變長,或是disconnect &lt;br /&gt;
 廣告投放要投在哪個點影響力最大,如果是傳染病隔離哪個點最有效?&lt;/li&gt;
  &lt;li&gt;Mixing patterns&lt;br /&gt;
 探討兩邊節點的type,可能因為什麼關係成為朋友(職業/興趣/文化)&lt;/li&gt;
  &lt;li&gt;Degree Correlations
 觀察兩邊點的degree&lt;br /&gt;
 內向和外向人(朋友多,degree高)觀察&lt;/li&gt;
  &lt;li&gt;Community Structure 
 一群點邊的密度很高,稱作一個community  &lt;br /&gt;
 clique 判斷是否認兩個點是否都有邊相連(clique problem 分團問題)&lt;br /&gt;
 clique problem 是 NP-Complete&lt;br /&gt;
 Connected commponets :有連通的子圖&lt;/li&gt;
  &lt;li&gt;Network motifs    &lt;br /&gt;
在音樂上motifs是一種作曲法,靈感的意思&lt;br /&gt;
 在生物基因上是一些重複的pattern&lt;br /&gt;
 在社群希望找到出現次數較高的motifs(最常出現的subgraph)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CERN
&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%B1%B3%E7%88%BE%E6%A0%BC%E5%80%AB%E5%AF%A6%E9%A9%97&quot;&gt;米爾格倫實驗 Milgram experiment&lt;/a&gt;服從威權實驗 &lt;br /&gt;
random graph&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Central of Network&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;找到最重要的點(central)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;local&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Degree&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;global&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Closeness&lt;/li&gt;
    &lt;li&gt;Betweeness&lt;/li&gt;
    &lt;li&gt;Eigenvector&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Group Centrality 一群最有影響力的人&lt;br /&gt;
在小世界理論中,如果送信到目標的前一步,都是經由特定的3個人,代表這三個人很重要,&lt;br /&gt;
目前social network還無法透過社群網站判斷這些人&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Social actors(群眾的智慧)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Connectors&lt;br /&gt;
認識很多人,很擅長社交&lt;/li&gt;
  &lt;li&gt;Mavens&lt;br /&gt;
資訊專家,知道很多各式訊息&lt;/li&gt;
  &lt;li&gt;Salesman&lt;br /&gt;
容易說服別人,擅長協調&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;判斷social network的四種centrality&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Degree centrality(local)&lt;br /&gt;
點的重要性,若network的規模大小不同,做normalize(除總size-1)&lt;/li&gt;
  &lt;li&gt;Betweeness Centrality&lt;br /&gt;
Node&lt;sub&gt;i&lt;/sub&gt; A到B的shortest path有幾條經過i&lt;/li&gt;
  &lt;li&gt;Closeness Centrality
點i和所有點j的shortest path平均的距離&lt;/li&gt;
  &lt;li&gt;Eigenvector Centrality  &lt;br /&gt;
這個點的重要性,看他朋友點的重要性&lt;br /&gt;
eigenvector
    &lt;blockquote&gt;
      &lt;p&gt;一個向量乘上一個矩陣(transform),方向不變但scale可能會變&lt;br /&gt;
Ax = &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;x&lt;br /&gt;
A矩陣代表social network關係(1:朋友關係,0:不是朋友)&lt;br /&gt;
x代表重要性&lt;br /&gt;
概念類似PageRank,page rank的值是連到他網頁的值加總&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;HIT &lt;br /&gt;
Hub&lt;br /&gt;
推薦的authoritative有多高&lt;br /&gt;
Authoritative page&lt;br /&gt;
有多少hub推薦&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最短路徑演算法&lt;/strong&gt;&lt;br /&gt;
unweighted graph&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;BFS&lt;/li&gt;
  &lt;li&gt;Floyd-Warshall&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Group centrality&lt;/strong&gt;&lt;br /&gt;
找出social network中幾個最有影響力的人&lt;br /&gt;
或指定某幾個人想觀察這幾人的影響力&lt;/p&gt;

&lt;p&gt;group centrality一群人一起看,影響幾個人(有連線)&lt;br /&gt;
不能將每個單一人的degree加總,會有重複的&lt;/p&gt;

&lt;p&gt;Social Group Analysis
community detection algorithm&lt;/p&gt;

&lt;!-- 20170413 start --&gt;
&lt;p&gt;&lt;strong&gt;Properties of cohesion 凝聚力的判斷&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Mutuality of ties&lt;br /&gt;
 所有subgroup彼此都有編相連,在graph中就是完全圖的概念&lt;br /&gt;
 e.g. clique&lt;/li&gt;
  &lt;li&gt;Closeness or reachability of subgroup members&lt;br /&gt;
 不需要直接有邊相連,間接有相連就行了&lt;br /&gt;
 e.g. N-clique,N-clan,N-club&lt;/li&gt;
  &lt;li&gt;Frquency of ties among members&lt;br /&gt;
 Mutuality of ties是說假設有n個人必須要和n-1個人相連,&lt;br /&gt;
 那Frquency of ties among members只需要和n-k個人相連就可以了&lt;br /&gt;
 是Mutuality of ties放寬版本&lt;br /&gt;
 e.g. K-plex,K-core&lt;/li&gt;
  &lt;li&gt;Relative frequency of ties among subgroup members compared to non-member&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Clique&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;maximal complete subgraph,最大的子圖任兩點都有邊相連&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/img/cloudcomputing/community01.png&quot; alt=&quot;clique img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N-clique &lt;br /&gt;
  在grahp中,任兩個點之間的距離&amp;lt;N  &lt;br /&gt;
  e.g. 2-cliques: {1,2,3,4,5},{2,3,4,5,6}&lt;/li&gt;
  &lt;li&gt;N-clan&lt;br /&gt;
  必須是N-clique&lt;br /&gt;
  在subgraph中,任兩個點之間的距離&amp;lt;N&lt;br /&gt;
  e.g. 2-clan: {2,3,4,5,6} &lt;br /&gt;
  (4-&amp;gt;5要經過6,但只考慮1,2,3,4,5這個subgraph)&lt;/li&gt;
  &lt;li&gt;N-club&lt;br /&gt;
  不必是N-clique,但一定要是subgraph of n-cliques  &lt;br /&gt;
  2-clubs: {1,2,3,4},{1,2,3,5},{2,3,4,5,6}&lt;/li&gt;
  &lt;li&gt;K-plex&lt;br /&gt;
  如果是clique每個點的degree是n-1&lt;br /&gt;
  如果是k-plex,每個點的degree是n-k&lt;br /&gt;
  假設subgraph有4個點,2-plex每個點的degree至少是2&lt;/li&gt;
  &lt;li&gt;K-core&lt;br /&gt;
  至少和k個人是朋友
  每個點的degree至少是k&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Community Detection Approaches&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Kernighan-Lin Alog(KL algorithm)&lt;/li&gt;
  &lt;li&gt;Hierarchical Clustering&lt;/li&gt;
  &lt;li&gt;Modularity Maximization&lt;/li&gt;
  &lt;li&gt;Bridge-Cut Algo&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;kl-algorithm&quot;&gt;KL algorithm&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;input: weighted graph&lt;br /&gt;
output: 切成兩個equal-size subgraph,且橫跨兩群的crossing edge &lt;br /&gt;
目的是相望群和群之間差異大,群內部的差異小&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;步驟&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;任意切成兩半&lt;/li&gt;
  &lt;li&gt;計算每一點的difference&lt;/li&gt;
  &lt;li&gt;計算每個邊的gain&lt;/li&gt;
  &lt;li&gt;從gain最大的開始做交換,交換後的點不再考慮(lock)&lt;/li&gt;
  &lt;li&gt;交換到直到全部的點都被lock住&lt;/li&gt;
  &lt;li&gt;挑gain總和最大的就是最終交換結果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;交換數回合,若遇到gain是負的紀錄下來並繼續嘗試做交換,到最後再找gain最好的
交換完後的點就lock住不進入下一回合&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;external cost&lt;br /&gt;
  crossing edge的cost(連向別群的cost)(cut-size)&lt;/li&gt;
  &lt;li&gt;internal cost&lt;br /&gt;
  連向同群的cost&lt;/li&gt;
  &lt;li&gt;difference&lt;br /&gt;
  external cost - internal cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Gain&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;用來評估是否要交換的值
例如a,b屬於不同群,ab做交換&lt;br /&gt;
Gain = &lt;script type=&quot;math/tex&quot;&gt;D_a + D_b - 2\times W_{ab}&lt;/script&gt;&lt;br /&gt;
(Difference a + Difference b - 2*weighted ab)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;若考慮a,b交換&lt;br /&gt;
old cost = &lt;script type=&quot;math/tex&quot;&gt;z + E_a + E_b - W_{ab}&lt;/script&gt; &lt;br /&gt;
new cost = &lt;script type=&quot;math/tex&quot;&gt;z + I_a + I_b + W_{ab}&lt;/script&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;z (與a,b沒有連接的其他crossing edge總和)&lt;br /&gt;
E (external cost)&lt;br /&gt;
I (internal cost)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;KL algorithm複雜度&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt; 找到最適合交換的兩點,有n pair要交換&lt;br /&gt;
⇒ &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt;&lt;/p&gt;

&lt;!-- 20170413 end --&gt;

&lt;p&gt;hiraichiecal 
bottom-up&lt;br /&gt;
每一回合都找兩個最像的做合併&lt;br /&gt;
single link&lt;br /&gt;
    距離取min
complete link
    距離取max&lt;/p&gt;

&lt;p&gt;分群    &lt;br /&gt;
community 同群邊的值要越大越好&lt;/p&gt;

&lt;p&gt;Distance Matrix&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Approach1  &lt;br /&gt;
計算weights Wij
    &lt;ul&gt;
      &lt;li&gt;i到j的路徑越多代表i和j關係越好&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;只能找non-overlapped paths&lt;/li&gt;
      &lt;li&gt;只要i到j的路徑都算(weighted by length)
 Xij = 1/Wij&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Reduction
     由A問題轉換到B問題&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Approach 2
 如果i和j視同一群,那他們有相似的behavior
 behavoir
     i和j到commuinty其他點的平均距離相似&lt;/li&gt;
  &lt;li&gt;Approach 3
 J(i,j)/min(Ki,Kj)&lt;br /&gt;
 看兩個人共同朋友個數,共同朋友越多J(i,j)越大&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Edge-removal Approach&lt;br /&gt;
不斷的拿掉邊,會出現越多的群數,直到符合要的群數&lt;br /&gt;
拿掉bridge edge,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;betweeness
一開始想說可以用degree少,但不夠完全&lt;br /&gt;
 在centrality的betweeness是以node考量&lt;br /&gt;
 在這的betweeness是以edge考量&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;GN algorithm&lt;br /&gt;
拿掉betweeness最高的邊 -&amp;gt; 重算betweeness -&amp;gt; 計算community
top-down(起始是一個commuinity,並分群下去)
計算邊的betweeness&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;shortest path&lt;br /&gt;
 任兩點最短路徑有多少條會經過邊&lt;/li&gt;
  &lt;li&gt;Random-walk 
 計算a會走到b的機率&lt;br /&gt;
 a走到b會經過邊v的機率&lt;/li&gt;
  &lt;li&gt;Current-flow
 引進電路學概念的計算方法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;缺點&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;計算最短路徑耗時
 O(m^2n)&lt;br /&gt;
 m edge (O(mn)betweeness)&lt;/li&gt;
  &lt;li&gt;什麼時候停?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;改善&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Partial betweeness (Apprximation)&lt;br /&gt;
Randomly sampled by Monte Carlo Estimate&lt;/li&gt;
  &lt;li&gt;Edge clustering coefficient
  coefficient越高代表關係越好
the smaller coefficient the higher betweeness&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;modularity&quot;&gt;Modularity&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Modularity measure:
    how good a particular partition forms a community.
    評估community切分的好不好  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;U 看internal edge的比例&lt;br /&gt;
R 平均i和j會有邊的機率(期望值)      &lt;br /&gt;
Q = U - R&lt;/p&gt;

&lt;p&gt;Q = 0 no community&lt;br /&gt;
Q ~ 1 prefect cut&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Newman Fast Alogorithm&lt;/strong&gt;&lt;br /&gt;
利用hireachcal合併,並每個步驟算modurity,並找出最高的Q做切分&lt;br /&gt;
&lt;strong&gt;Bridge cut&lt;/strong&gt;&lt;br /&gt;
integrity一致性&lt;br /&gt;
N(v)
d(v): degree of node
Density
Direct neighbor subgraph of v&lt;br /&gt;
Clustering coeffiecient &lt;br /&gt;
	觀察v的鄰居的朋友關係&lt;br /&gt;
	例如v有4個朋友,那4個人最多有6個關係,算關係的比例&lt;br /&gt;
	實際上有關係/最多有幾個關係&lt;br /&gt;
Bridge Centrality&lt;br /&gt;
	&lt;strong&gt;rank&lt;/strong&gt; of betweenness centrality * &lt;strong&gt;rank&lt;/strong&gt; of bridging coeffiecient&lt;br /&gt;
	如果只考慮betweeness(global)會有一些情況不太好  &lt;br /&gt;
	加入bridge centrality可以考慮到local的特質&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Community Search&lt;/strong&gt;&lt;br /&gt;
給一個social network,並給一些query(其中幾個人),&lt;br /&gt;
given grahp G, a set of query node&lt;br /&gt;
goal: find a densely subgraph of G, and contains the query nodes&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Induced Subgraph&lt;/strong&gt;&lt;br /&gt;
xy edge在G中,xy edge也要在induce subgraph中&lt;/p&gt;

&lt;p&gt;goodness function&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;edge degree
時間複雜度太大&lt;/li&gt;
  &lt;li&gt;average degree&lt;/li&gt;
  &lt;li&gt;minumin degree
這群人認識最少的人,讓這個人的值變大
induced subgraph的degree&lt;br /&gt;
容易受到outlier影響&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Constrain&lt;br /&gt;
distance constrain&lt;br /&gt;
限制邀請來的人的最長距離&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monotone Function&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monotone increaing&lt;/li&gt;
  &lt;li&gt;monotone decresing&lt;/li&gt;
  &lt;li&gt;non-monotone&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;!-- 20170421 start --&gt;
&lt;blockquote&gt;

  &lt;ul&gt;
    &lt;li&gt;Link Prediction&lt;/li&gt;
    &lt;li&gt;Node-wise Similarity Based Methods&lt;/li&gt;
    &lt;li&gt;Topological Pattern Based Methods&lt;/li&gt;
    &lt;li&gt;Probabilistic Model Based Methods&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;link-prediction&quot;&gt;Link Prediction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Predict the existence of links&lt;/li&gt;
  &lt;li&gt;Predict the type of links&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Strategies of Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Knowledge-driven strategy&lt;br /&gt;
 專家系統(領域專家提供rule)&lt;/li&gt;
  &lt;li&gt;Data-driven approach&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;problem&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Link existence prediction&lt;br /&gt;
 邊是否存在&lt;/li&gt;
  &lt;li&gt;Link classification&lt;br /&gt;
 關係的總類&lt;/li&gt;
  &lt;li&gt;Link regression&lt;br /&gt;
 最重要的邊是哪一個&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Web hyperlink creation&lt;/li&gt;
  &lt;li&gt;Collaborative filitering&lt;/li&gt;
  &lt;li&gt;Information retrieval&lt;/li&gt;
  &lt;li&gt;Clustering&lt;/li&gt;
  &lt;li&gt;Record linkage&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;node-wise-similarity-based-method&quot;&gt;Node-wise Similarity Based Method&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;計算兩個點的相似度,如果兩個點很相似他們可能就有link&lt;br /&gt;
e.g. Similarity between words&lt;br /&gt;
觀察word的前後文字來判斷相似程度&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Learning-Based Similarity Measure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Binary Classification Approach
    &lt;ul&gt;
      &lt;li&gt;Decision Tree&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Regression-based Approach(回歸)  &lt;br /&gt;
  e.g linear regression&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;Y = \alpha + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n&lt;/script&gt;&lt;br /&gt;
  利用學習方式估計出&lt;script type=&quot;math/tex&quot;&gt;\alpha , \beta&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topological-pattern-based-methods&quot;&gt;Topological Pattern Based Methods&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;計算兩點之間的分數,若大於某個值就表示他們之間有關係(連線)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Local Method&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Common Neighbors(CN)  &lt;br /&gt;
  計算共同的鄰居&lt;/li&gt;
      &lt;li&gt;Salton Index&lt;br /&gt;
  類似cosine similiary&lt;/li&gt;
      &lt;li&gt;Jaccard Coefficient(JC)&lt;br /&gt;
  交集/聯集&lt;/li&gt;
      &lt;li&gt;Leicht-Holme-Newman Index(LHN)&lt;/li&gt;
      &lt;li&gt;Hub Promoted Index(HPI)&lt;br /&gt;
  Hub概念像是入口網站,類似目錄連到很多子分支&lt;/li&gt;
      &lt;li&gt;Hub Depres Index(HDI)&lt;/li&gt;
      &lt;li&gt;Adamic/Adar(AA) &lt;br /&gt;
  x和y是朋友的分數是x和y的共同朋友的鄰居-x和y 倒數總和&lt;/li&gt;
      &lt;li&gt;Resource Allocation Index(RA)&lt;br /&gt;
  和AA差在分母沒有取log&lt;/li&gt;
      &lt;li&gt;Preferential Attachment&lt;br /&gt;
  x和y是朋友的分數就是x的鄰居乘上y鄰居&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Performance: RA &amp;gt; AA &amp;gt; CN &amp;gt; … &amp;gt; PA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;global Method&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;katz&lt;br /&gt;
  看x到y距離是1,2,3..n的path有幾條,乘上一個參數做加總&lt;/li&gt;
      &lt;li&gt;Hitting Time&lt;br /&gt;
  x走到y,做random work的期望值作為比較條件&lt;/li&gt;
      &lt;li&gt;PageRank&lt;/li&gt;
      &lt;li&gt;SimRank&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;probabilistic-model-based-methods&quot;&gt;Probabilistic Model Based Methods&lt;/h2&gt;
&lt;p&gt;e.g. relational Markov model&lt;br /&gt;
&lt;!-- 20170421 end --&gt;&lt;/p&gt;

&lt;!-- 20170420 class--&gt;
&lt;h2 id=&quot;labeld-social-network&quot;&gt;Labeld Social Network&lt;/h2&gt;
&lt;p&gt;Type of Labels&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Binary&lt;/li&gt;
  &lt;li&gt;Numeric&lt;/li&gt;
  &lt;li&gt;cate&lt;/li&gt;
  &lt;li&gt;text-free&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;label-prediction&quot;&gt;Label Prediction&lt;/h2&gt;
&lt;p&gt;根據已知的label預測未知點的label&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Inference vs. Learning&lt;br /&gt;
 Inference(unsupervised)&lt;br /&gt;
 Learning(supervised)&lt;br /&gt;
2.Disjoint vs. Collective&lt;br /&gt;
 Disjoint
 沒有標籤的點就不考慮&lt;br /&gt;
 Collective&lt;br /&gt;
 沒有標籤的點也會放進去考慮&lt;br /&gt;
3.Across-network vs. within-network learning&lt;br /&gt;
 Across-network拿一個social network model去預測另一個social network&lt;br /&gt;
 within-network拿全部資料做的model來做預測&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;clues-to-predict-labels&quot;&gt;Clues to Predict Labels&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Label Independent approaches 特徵值沒有用到label的訊息
    &lt;ol&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
  Attribute of node i (年紀)
  Label of node i (身份)&lt;/li&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
  Network Structures of node i (between centrality)
  Label of node i&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Label Dependent Approaches
    &lt;ol&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
  知道鄰居來預設未知,用其他點來預測未知點&lt;/li&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
  利用unlabel點來預測&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Relational Neighbor Classifier&lt;br /&gt;
    -&amp;gt; disjoint
    簡單說就是看鄰居多數是什麼就判斷node是什麼&lt;br /&gt;
    問題：&lt;br /&gt;
    如果已知的點很少,unknown的很多(Sparse label),若只用一個點就判斷就沒那麼可靠&lt;br /&gt;
    解決：
    Iterative Relation Neighbor classifier  &lt;br /&gt;
    判斷分為好幾回合,若多數點是unknown那就判斷是unknown &lt;br /&gt;
    unknown也是一種label&lt;br /&gt;
&lt;!-- 20170420 class--&gt;&lt;/p&gt;

&lt;!-- 20170421 start --&gt;
&lt;p&gt;&lt;strong&gt;Ghost Edges for Node Label Prediction&lt;/strong&gt;&lt;br /&gt;
將一些不是直接連接的node但有影響力的點,用ghost edge連起來&lt;br /&gt;
那怎麼判斷點的重要性&lt;br /&gt;
-&amp;gt; Random walk with Restart &lt;br /&gt;
    有一定的機率會跳到起點&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Steady-state Probability&lt;/strong&gt;&lt;br /&gt;
Markov Process  &lt;br /&gt;
    城市和郊區遷移問題&lt;/p&gt;

&lt;p&gt;利用Random walk with restart計算所有點對某點的影響力&lt;br /&gt;
在對這些影響力做等級劃分,依照機率分為ABCDEF…等級&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two Classifers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;GhostEdgeNL&lt;/li&gt;
  &lt;li&gt;GhostEdgeL
 Logistic regression&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 20170421 end --&gt;
</content>
 </entry>
 

</feed>
