<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Note</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2017-06-02T14:09:04+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Taiyi</name>
   <email>moved0311@gmail.com</email>
 </author>

 
 <entry>
   <title>Data Structure</title>
   <link href="http://localhost:4000/2017/dataStructure/"/>
   <updated>2017-03-27T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/dataStructure</id>
   <content type="html">&lt;h2 id=&quot;linklist&quot;&gt;LinkList&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;linklist reverse&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;/img/dataStructure/linkListReverse.png&quot; alt=&quot;linkListReverse&quot; /&gt;
&lt;!--more--&gt;
    &lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;linkListReverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; 
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;   
              &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;null&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;  
             &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
             &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
          &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;){&lt;/span&gt;
        
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++){&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;before reverse:&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverseHead&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;After reverse:&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reverseHead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>WebSearching</title>
   <link href="http://localhost:4000/2017/webSearching/"/>
   <updated>2017-03-07T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/webSearching</id>
   <content type="html">&lt;hr /&gt;

&lt;p&gt;課本: &lt;a href=&quot;https://nlp.stanford.edu/IR-book/&quot;&gt;Introduction to Information Retrieval&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;ch8-evaluation-in-information-retrival&quot;&gt;CH8 Evaluation in information retrival&lt;/h1&gt;
&lt;p&gt;評量search engine好壞&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;搜到的index&lt;/li&gt;
  &lt;li&gt;搜尋速度&lt;/li&gt;
  &lt;li&gt;二氧化碳排放量&lt;/li&gt;
  &lt;li&gt;和搜尋相關程度&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;
&lt;p&gt;相關程度&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;benchmark資料集&lt;/li&gt;
  &lt;li&gt;benchmark queries(問句)&lt;/li&gt;
  &lt;li&gt;文章是否相關的標記(ground truths)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;queries和information need有落差
想找的東西,不會下key word&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision (P)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Precision = &lt;script type=&quot;math/tex&quot;&gt;\frac{檢索到相關物件的數量}{物件總數}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Recall (R)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Recall = &lt;script type=&quot;math/tex&quot;&gt;\frac{檢索到相關物件的數量}{相關物件總數}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Relevant&lt;/td&gt;
      &lt;td&gt;Nonrelevent&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Retrieved&lt;/td&gt;
      &lt;td&gt;true positive(tp)&lt;/td&gt;
      &lt;td&gt;false positive(fp)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Not retrieved&lt;/td&gt;
      &lt;td&gt;false negatives(fn)&lt;/td&gt;
      &lt;td&gt;true negatives(tn)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;true positive(tp) 機器判斷+且為真&lt;br /&gt;
false positive(fp)機器判斷+但是是假&lt;br /&gt;
false negatives(fn) 機器判斷-但判斷錯&lt;br /&gt;
true negatives(tn)  機器判斷-且判斷對&lt;/p&gt;

&lt;p&gt;P = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp}{tp+fp}\&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;R = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp}{tp+fn}\&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Accuracy vs Precision&lt;/strong&gt; &lt;br /&gt;
accuracy = &lt;script type=&quot;math/tex&quot;&gt;\frac{tp+tn}{tp+fp+fn+tn}\&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;accuracy不適合用在information retrieval,
因為通常Nonrelevant會非常的大,tn項非常大除分母fn和tn都非常大結果會趨近於1,所以才用Precision和Recall作為依據&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;調和平均數(harmonic mean)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;H = &lt;script type=&quot;math/tex&quot;&gt;\frac{n}{\frac{1}{x_1}+\frac{1}{x_2}+..+\frac{1}{x_n}}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Precision/Recall Tradoff
使用調和平均數計算Precision和Recall的Tradoff&lt;br /&gt;
量測的數值稱做F measure,α和1-α分別為P和R的權重,一般是取α=0.5&lt;br /&gt;
(P和R重要程度相同)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;F = &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{\alpha\frac{1}{P}+(1-\alpha)\frac{1}{R}} = \frac{(\beta^2+1)PR}{\beta^2P+R}\&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\ \beta^2=\frac{1-\alpha}{\alpha}\&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;ranked-evalution&quot;&gt;Ranked Evalution&lt;/h4&gt;
&lt;p&gt;P,R,F都是unordered(沒有等級)&lt;br /&gt;
一個query會有一個Precision/Recall圖    &lt;br /&gt;
使用內插法(interpolated)可以得到一張較平滑的P-R圖&lt;br /&gt;
(和機器學習ROC curve相似)&lt;br /&gt;
P-R curve的面積越大效能越佳(代表Precision掉越慢)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;內插法&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ p_{interp}= \max\limits_{r'\ge r}\ p(r{'})\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;r代表recall,
作法是從目前往後找最高的點向前填平,並重新畫P-R圖&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean Average Precision(MAP)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ MAP(Q) = \frac{1}{|Q|}\sum^{|Q|}_{j=1}\frac{1}{m_j}\sum_{k=1}^{m_j}Precision(R_{jk})\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;第一個sum算query平均&lt;br /&gt;
第二個sum算precision平均&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision at k&lt;/strong&gt;&lt;br /&gt;
第k個搜索結果的Precision&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-Precision&lt;/strong&gt;&lt;br /&gt;
文件中總共有R篇相關文章,以R作為cut-off,計算Precision&lt;br /&gt;
e.g. 總共有100篇文章,其中10篇是相關的&lt;br /&gt;
且搜尋結果是:RNRNN RRNNN RNNRR ….&lt;br /&gt;
R=10(只看RNRNN RRNNN)計算Precision&lt;br /&gt;
R-Precision = 0.4&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Normalized Discounted Cumulative Gain(NDCG)&lt;/strong&gt;  &lt;br /&gt;
作者：Kalervo Jarvelin, Jaana Kekalainen(2002)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;用來衡量ranking quality&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;e.g.&lt;br /&gt;
G = &amp;lt;3,2,3,0,0,1,2,2,3,0,…&amp;gt; &lt;br /&gt;
G表示一個搜索的結果(3高度相關, 0沒關係)&lt;br /&gt;
步驟:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cumulative Gain(CG)
    &lt;blockquote&gt;

      &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\
 CG[i] = \left\{\begin{matrix}
 G[1], &amp;if\ i=1 \\ 
 CG[i-1]+G[i], &amp;otherwise 
 \end{matrix}\right.
 \ %]]&gt;&lt;/script&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;目前項+＝前一項(做成一個遞增的函數)&lt;br /&gt;
 CG’=&amp;lt;3,5,8,8,8,9,11,13,16,16,…&amp;gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Discounted Cumulative Gain(DCG)
    &lt;blockquote&gt;

      &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\
 DCG[i]=\left\{\begin{matrix}
 G[i], &amp; if\ i=1\\ 
 DCG[i-1]+G[i]/log_b\ i, &amp; otherwise
 \end{matrix}\right.
 \ %]]&gt;&lt;/script&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;DCG’=&amp;lt;3,5,6.89,6.89,6.89,7.28,7.99,8.66,9.61,9.61,…&amp;gt; if b=2&lt;br /&gt;
 i代表排名,對排名做懲罰(除log&lt;sub&gt;2&lt;/sub&gt; i),排名越後面懲罰越重&lt;br /&gt;
 代表如果搜尋的結果很差,和理想的排序分數會相差很多&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Normalized Discounted Cumulative Gain(NDCG)&lt;br /&gt;
 理想的搜索結果I=&amp;lt;3,3,3,2,2,2,1,1,1,1,0,0,0,…&amp;gt;(高度相關的排越前面) &lt;br /&gt;
 理想搜索結果DCGI=&amp;lt;3,6,7.89,8.89,9.75,10.52,10.88,11.21,…&amp;gt;&lt;br /&gt;
 nDCG&lt;sub&gt;n&lt;/sub&gt; = &lt;script type=&quot;math/tex&quot;&gt;\ \frac{DCG_{n}}{IDCG_{n}}(\frac{相關程度排序}{理想相關程度},做正規化)\&lt;/script&gt;&lt;br /&gt;
 NDCG=&amp;lt;1,0.83,0.87,0.77,0.70,0.69,0.73,0.77,…&amp;gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;benchmark 資料集&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cranfield&lt;/li&gt;
  &lt;li&gt;TREC(nist.gov)&lt;br /&gt;
 Ad-hoc 資料集(1992-1999)&lt;/li&gt;
  &lt;li&gt;GOV2
 2500萬篇文章&lt;/li&gt;
  &lt;li&gt;NICIR
 cross-language IR&lt;/li&gt;
  &lt;li&gt;Cross Language Evaluation&lt;/li&gt;
  &lt;li&gt;REUTERS&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;標記資料準則&lt;/strong&gt; &lt;br /&gt;
Kappa measure&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;標記資料是否一致的衡量標準,若標記不一致資料中就沒有truth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kappa計算公式&lt;/p&gt;
&lt;blockquote&gt;
  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\ \kappa = \frac{P(A)-P(E)}{1-P(E)}\&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;搜尋結果的呈現 Result Summaries&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;搜尋結果呈現：10 blue link&lt;/li&gt;
  &lt;li&gt;搜尋結果下方文字說明分為Static和Dynamic
Static:固定抽前50個字
Dynamic:利用nlp技術,根據搜索關鍵字動態做變化&lt;/li&gt;
  &lt;li&gt;quicklinks&lt;br /&gt;
底下多的連結&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;ch6-model&quot;&gt;CH6 Model&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Vector Space Model&lt;/li&gt;
  &lt;li&gt;Probabilistic Information Retrieval&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Empirical IR&lt;/td&gt;
      &lt;td&gt;Model-based IR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;暴力法&lt;/td&gt;
      &lt;td&gt;有理論模型&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;heuristic&lt;/td&gt;
      &lt;td&gt;數學假設&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;難推廣到其他問題&lt;/td&gt;
      &lt;td&gt;容易推廣到其他問題&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;IR model歷史&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1960&lt;br /&gt;
  第一個機率模型&lt;/li&gt;
  &lt;li&gt;1970   &lt;br /&gt;
  vector space model(75)  &lt;br /&gt;
  classic probabilistic model(76)&lt;/li&gt;
  &lt;li&gt;1980&lt;br /&gt;
  non-class logic model(86)&lt;/li&gt;
  &lt;li&gt;1990&lt;br /&gt;
  TREC benchmark &lt;br /&gt;
  BM25/Okapi(94)&lt;br /&gt;
  google成立(96)  &lt;br /&gt;
  Language model(98)&lt;/li&gt;
  &lt;li&gt;2000&lt;br /&gt;
  Axiomatic model(04)&lt;br /&gt;
  Markov Random Field(05)   &lt;br /&gt;
  Learning to rank(05)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Vector space&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Vocabulary&lt;/td&gt;
      &lt;td&gt;V = { &lt;script type=&quot;math/tex&quot;&gt;w_1,w_2,w_3,...w_v&lt;/script&gt; }&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Query&lt;/td&gt;
      &lt;td&gt;q =&lt;script type=&quot;math/tex&quot;&gt;\{q_1,q_2,...,q_m\}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Document 文章&lt;/td&gt;
      &lt;td&gt;&lt;script type=&quot;math/tex&quot;&gt;{d_i} = \{  w_1,w_2,...  \}&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Collection文章集合&lt;/td&gt;
      &lt;td&gt;C = { &lt;script type=&quot;math/tex&quot;&gt;d_1,d_2,d_3,...&lt;/script&gt; }&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;R(q) query的集合&lt;/td&gt;
      &lt;td&gt;R(q) ⊂ C&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;目標是找到近似query的集合&lt;/strong&gt;&lt;br /&gt;
策略:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Document select&lt;br /&gt;
 挑文件如果是相關就收到集合&lt;br /&gt;
 absolute relevance(系統必須決定是相關還是不相關)&lt;/li&gt;
  &lt;li&gt;Document ranking&lt;br /&gt;
 query的結果&amp;gt;threshold 就收進去  &lt;br /&gt;
 relative relevance(不必是1或0,相近到一定程度就收進集合)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Probability Ranking Principle(PRP)&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Robertson (1977)&lt;br /&gt;
相似度量測函數f滿足,&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;f(q,d_1) &gt; f(q,d_2)\quad iff\quad p(Rel|q,d_1) &gt; p(Rel|q,d_2)&lt;/script&gt;  &lt;br /&gt;
f()值越大表示有越大的機率越相似&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Relevance流派&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Similarity 相似度&lt;br /&gt;
   Vector space model(Salton et al,75)&lt;/li&gt;
  &lt;li&gt;Probability of relevance 機率模型
  Classical probaility Model(Robertson&amp;amp;Sparck Jones,76)&lt;br /&gt;
  Learning to Rank(Joachims,02, Berges et al,05)&lt;/li&gt;
  &lt;li&gt;Probability inference 機率推論&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Vector Space Model(VSM)&lt;/strong&gt;&lt;br /&gt;
將query和document表示成向量形式(similar representation)&lt;br /&gt;
假設Relevance(d,q) = similar(d,q)&lt;br /&gt;
利用cosine算相似度(1 ~ -1) &lt;br /&gt;
high dimension(index的維度通常在10萬左右) &lt;br /&gt;
good dimension -&amp;gt; orthogonal&lt;br /&gt;
(好的維度切割應該是維度間彼此獨立(orthogonal),&lt;br /&gt;
但是通常很困難,例如nccu後面接university的機率很高)&lt;br /&gt;
VSM優點: Empirically effective,直觀, 實作容易&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;VectorSpace範例程式&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;http://blog.josephwilk.net/projects/building-a-vector-space-search-engine-in-python.html&quot;&gt;Building a Vector Space Search Engine in Python&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;大致步驟&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;將所有文章使用join()成為一個string包含所有文章內容&lt;/li&gt;
  &lt;li&gt;做string clean去除&lt;code class=&quot;highlighter-rouge&quot;&gt;.&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;,&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;多餘空白&lt;/code&gt;,並轉為小寫&lt;/li&gt;
  &lt;li&gt;將clean好的string利用空白切分成words array,丟到&lt;a href=&quot;https://tartarus.org/martin/PorterStemmer/&quot;&gt;Porter stem&lt;/a&gt;(去除字尾)
    &lt;blockquote&gt;
      &lt;p&gt;Porter Stemming Algorithm&lt;br /&gt;
    作者:Martin Porter(2006)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;刪除重複的word,使用set讓出現的word唯一&lt;/li&gt;
  &lt;li&gt;得到所有整理完的words,做成index(將每個word編號),類似字典的概念&lt;/li&gt;
  &lt;li&gt;將每篇文章分別建立自己的vector,並統計每個word出現的次數(term frequecy)&lt;/li&gt;
  &lt;li&gt;將輸入的query做成vector&lt;/li&gt;
  &lt;li&gt;利用兩個向量做cosine計算相關程度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;相似度計算&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Cosine Similarity
    &lt;blockquote&gt;
      &lt;p&gt;cosine = &lt;script type=&quot;math/tex&quot;&gt;\frac{V_1 \cdot V_2}{\|V_1\|\|V_2\|}&lt;/script&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Jaccard Similarity
    &lt;blockquote&gt;
      &lt;p&gt;相似度 = &lt;script type=&quot;math/tex&quot;&gt;\frac{交集}{聯集}&lt;/script&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;TF-IDF Weighting&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TF(Term Frequency)&lt;br /&gt;
  word count,單純統計字數出現頻率&lt;/li&gt;
  &lt;li&gt;IDF(Inverse Document Frequency)(反向的TF)&lt;br /&gt;
  字的獨特性,如果某些字在很多篇文章出現次數都很高(例如:the,a,to,…) &lt;br /&gt;
  IDF值就會很低(沒有鑑別度)	&lt;br /&gt;
  IDF(t) = 1 + log(n/k)  (n:篇數,k:字出現次數)&lt;br /&gt;
  例如文章總數是1000(n=1000),所有文章都有出現cat(k=1000),&lt;br /&gt;
  IDF = 1 + log(1000/1000) = 1&lt;br /&gt;
  如果只有1篇文章有出現cat,&lt;br /&gt;
  IDF = 1 + log(1000/1) = 4&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TF-IDF計算方法:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;weight(t,d) = TF(t,d) * IDF(t)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;TF-IDF範例程式&lt;/strong&gt;&lt;br /&gt;
&lt;a href=&quot;http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/&quot;&gt;Tutorial: Finding Important Words in Text Using TF-IDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;ch11-probabilistic-information-retrieval&quot;&gt;Ch11 Probabilistic Information Retrieval&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Probability theory&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Joint probability &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A\cap B)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Conditional probability  &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A | B)&lt;/script&gt;&lt;br /&gt;
  probability of A given that event B occurred.&lt;/li&gt;
  &lt;li&gt;Chain rule &lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(A,B) = P(A\cap B) = P(A|B)P(B) = P(B|A)P(A)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Partition rule&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;P(B) = P(A,B) + P(\bar{A},B)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;Chain rule example wiki&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有兩個甕第一個甕放1個黑球2個白球,第二個甕放1個黑球3個白球&lt;br /&gt;
事件A是選到第一個甕,事件B是選到白球&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(B|A)= \frac{2}{3}&lt;/script&gt; 在選到第一個甕的情況下拿到白球&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A,B)=P(B|A)P(A)=\frac{2}{3} \times \frac{1}{2}&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Bayes' Rule&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(A|B) = \frac{P(B|A)P(A)}{P(B)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;from chain rule: &lt;script type=&quot;math/tex&quot;&gt;P(A|B)P(B) = P(B|A)P(A)&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;P(A)    : 事前機率(prior probability)  
P(A|B)  : 事後機率(postior probability)  
P(B|A)  : likelihood   

The term likelihood is just a synonym of probability.   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Odds&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(A) = \frac{P(A)}{P(\bar{A})} = \frac{P(A)}{1-P(A)}&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;an event provide a kind of multiplier for how probabilities change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Probability of Relevance&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Random variables:
    &lt;ul&gt;
      &lt;li&gt;query Q&lt;/li&gt;
      &lt;li&gt;document D&lt;/li&gt;
      &lt;li&gt;relevance R ∈ {0,1} &lt;br /&gt;
(1:相關,0:不相關)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Goal: P(R=1|Q,D) to rank relevant&lt;br /&gt;
  利用query和document相似度的機率做排名&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Refining P(R=1|Q,D) Methods&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Conditional Models(Discriminative Models)
    &lt;ul&gt;
      &lt;li&gt;利用各種方法找出機率P = f(x)&lt;/li&gt;
      &lt;li&gt;利用資料訓練參數&lt;/li&gt;
      &lt;li&gt;利用model去排列未知的document&lt;br /&gt;
 e.g. Learning to rank,類神經網路,…&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generative Models
    &lt;ul&gt;
      &lt;li&gt;compute the odd of O(R=1|Q,D) using Bayes' rules&lt;br /&gt;
 先找出資料的分佈再做預測&lt;/li&gt;
      &lt;li&gt;How to define P(Q,D|R)
        &lt;ul&gt;
          &lt;li&gt;Document generation: P(Q,D|R)=P(D|Q,R)P(Q|R) &lt;br /&gt;
  query放到條件 (e.g RSJ model)&lt;/li&gt;
          &lt;li&gt;Query generation: P(Q,D|R)=P(Q|D,R)P(D|R) &lt;br /&gt;
  document放到條件 (e.g language model)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;rsj-model-binary-independence-model&quot;&gt;RSJ Model &lt;a href=&quot;https://en.wikipedia.org/wiki/Binary_Independence_Model&quot;&gt;(Binary Independence Model)&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;利用&lt;strong&gt;Odd&lt;/strong&gt;值做ranking的依據:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(R|D,Q) = \frac{P(R=1|D,Q)}{P(R=0|D,Q)} = \frac{ \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q)} }{ \frac{P(D|Q,R=0)P(R=0|Q)}{P(D|Q)} } = \frac{P(D|Q,R=1)P(R=1|Q)}{P(D|Q,R=0)P(R=0|Q)} \quad(1)&lt;/script&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{P(R=1|Q)}{P(R=0|Q)}&lt;/script&gt; 
對document ranking沒有影響,視為常數&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{P(D|Q,R=1)}{P(D|Q,R=0)} = \prod_{t=1}^{M} \frac{P(D_t|Q,R=1)}{P(D_t|Q,R=0)} \quad(2)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;將document拆成多個獨立的document term連乘積,且&lt;script type=&quot;math/tex&quot;&gt;D_t \in \{0,1\}&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t=1}^{M} \frac{P(D_t|R=1,Q)}{P(D_t|R=0,Q)} \quad(3)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;(2)代入(1)可以整理出(3)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=1}^{M} \frac{P(D_t = 1|Q,R=1)}{P(D_t = 1|Q,R=0)} \cdot \prod_{t:D_t=0}^{M} \frac{P(D_t = 0|Q,R=1)}{P(D_t = 0|Q,R=0)}\quad(4)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;將document term分為出現或是不出現,(3)→(4)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_t = P(D_t = 1|Q,R=1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_t = P(D_t = 1|Q,R=0)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p_t&lt;/script&gt; 表示term出現在document且和query相關的機率&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;u_t&lt;/script&gt; 表示term出現在doucment且和query不相關的機率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t}{u_t} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \quad(5)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;假設&lt;script type=&quot;math/tex&quot;&gt;Q_t = 0 \; then \; p_t = u_t&lt;/script&gt;(假設可以做改變)  &lt;br /&gt;
意思是沒出現在query的term就不用考慮,只考慮&lt;script type=&quot;math/tex&quot;&gt;Q_t = 1&lt;/script&gt;  &lt;br /&gt;
左邊連乘積表示 query term found in document&lt;br /&gt;
右邊連乘積表示query term not found in document&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:D_t=0,Q_t=1} \frac{1-p_t}{1-u_t} \cdot \frac{1-p_t}{1-u_t} \quad(6)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;右邊連乘積乘上 &lt;script type=&quot;math/tex&quot;&gt;\frac{1-p_t}{1-u_t}&lt;/script&gt;, 所以所邊必須要除&lt;script type=&quot;math/tex&quot;&gt;\frac{1-p_t}{1-u_t}&lt;/script&gt;才會相等&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O(R|D,Q) = O(R|Q) \cdot \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} \cdot \prod_{t:Q_t=1} \frac{1-p_t}{1-u_t} \quad(7)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;右邊連乘積是query not found in document&lt;br /&gt;
概念大概是將query found in document也計算進去,&lt;br /&gt;
不管有沒有出現在document都乘&lt;br /&gt;
整理後右邊連乘積的範圍就會和document無關 &lt;br /&gt;
在對document ranking時就視為常數&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;RSV_d = log \prod_{t:D_t=Q_t=1} \frac{p_t(1-u_t)}{u_t(1-p_t)} = \sum_{t:D_t=Q_t=1} log \frac{p_t(1-u_t)}{u_t(1-p_t)} \quad(8)&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;取log後就得到Retrieval Status Value(RSV),&lt;br /&gt;
log是monotonic function不會改變ranking順序&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;RSJ Model:No Relevance Info&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{p_t(1-u_t)}{u_t(1-p_t)}&lt;/script&gt;

&lt;p&gt;如果沒有給relevance judgements,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;assume &lt;script type=&quot;math/tex&quot;&gt;p_t&lt;/script&gt; to be a constant&lt;/li&gt;
  &lt;li&gt;Estimate &lt;script type=&quot;math/tex&quot;&gt;u_t&lt;/script&gt; by assume all documents to be non-relevant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1979 Croft&amp;amp;Harper&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log O(R=1|Q,D) \approx  
\sum_{t=1,D_t=Q_t=1}^{k} log \frac{N - n_t + 0.5}{n_t + 0.5}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;N: number of documents in collection&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;n_t&lt;/script&gt; : number of documents in which term &lt;script type=&quot;math/tex&quot;&gt;D_t&lt;/script&gt; occurs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum log( \frac{總文章數 - 某個字出現在文章次數 + 0.5}{某個字出現在文章次數 + 0.5})&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;只看在document中和query相關的字,並加總每個字算出來的值&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;RSJ Model: with Relevance Info&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Maximum Lieklihood Estimate(MLE)&lt;/li&gt;
  &lt;li&gt;Maximum A Posterior(MAP)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;RSJ model的performance還遠比不上vector space model&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Improving RSJ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;adding TF&lt;/li&gt;
  &lt;li&gt;adding Doc.length&lt;/li&gt;
  &lt;li&gt;query TF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改善後的最終公式稱作&lt;strong&gt;BM25&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;!-- 20170416 --&gt;
&lt;h1 id=&quot;ch12-language-models-for-information-retrieval&quot;&gt;CH12 Language models for information retrieval&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;unigram language model&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;每個word只有單一的狀態,可以建立一個table放每個word對應到的機率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一個string出現的機率就是每個word的機率連乘積 &lt;br /&gt;
Language model應用：語音系統的語言校正&lt;/p&gt;

&lt;p&gt;Language model屬於query generation process  &lt;br /&gt;
每一篇document視為一個language model&lt;br /&gt;
ranking的計算是根據P(Q|D)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;計算P(Q|D)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;Multinomial model&lt;/a&gt; &lt;br /&gt;
  &lt;img src=&quot;/img/websearching/MultinomialDistribution.png&quot; alt=&quot;multinomial Distribustion&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(q|M_d) = P((t_1,...,t_{|q|})|M_d) = \prod_{1 \leq k \leq |q|} P(t_k|M_d) = \prod_{distinct\;term\;t\;in\;q}P(t|M_d)^{tf_{t,q}}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;|q|: length of query&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;t_k&lt;/script&gt; : query的第k個位置的token&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;tf_{t,q}&lt;/script&gt; : term frequency of t in q&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;估計參數&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;Maximun Likelihood Estimation(MLE)&lt;/a&gt;
    &lt;blockquote&gt;
      &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{P}(t|M_d) = \frac{tf_{t,d}}{|d|}&lt;/script&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hat符號表示估計值的意思  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;smooth the estimates to avoid zeros&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;避免0產生,相乘後結果很差&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;smooth方法&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Mixture model&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;P(t|d)=\lambda P(t|M_d) + (1-\lambda )P(t|M_c)&lt;/script&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;M_c&lt;/script&gt; : the collection model&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; 的設定好壞會影響效能&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;e.g.&lt;br /&gt;
 Collection = {&lt;script type=&quot;math/tex&quot;&gt;d_1,d_2&lt;/script&gt;}&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt;: Jack wants to play game&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt;: Tom is cat&lt;br /&gt;
 query q: Tom game&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\lambda = \frac{1}{2}&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;P(d|&lt;script type=&quot;math/tex&quot;&gt;d_1&lt;/script&gt;) = [(0/5 + 1/8)/2]&lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;[(1/5 + 1/8)/2] &lt;script type=&quot;math/tex&quot;&gt;\approx&lt;/script&gt; 0.0101 &lt;br /&gt;
 P(d|&lt;script type=&quot;math/tex&quot;&gt;d_2&lt;/script&gt;) = [(1/3 + 1/8)/2]&lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;[(0/3 + 1/8)/2] &lt;script type=&quot;math/tex&quot;&gt;\approx&lt;/script&gt; 0.0143&lt;br /&gt;
 rank &lt;script type=&quot;math/tex&quot;&gt;d_2 &gt; d_1&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=gCI-ZC7irbY&quot;&gt;Laplace smoothing&lt;/a&gt;
&lt;!-- 20170416  --&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 20170427 --&gt;
&lt;p&gt;&lt;strong&gt;Text Generation with Unigram LM&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sampling  &lt;br /&gt;
  由一個特定主題的model,裡面會有各個字出現的機率(每一個model會有一個distribution,機率分佈),取出一些機率較高的字可以形成document&lt;/li&gt;
  &lt;li&gt;estimation&lt;br /&gt;
  拿到一個document,預估出model&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- 20170427 --&gt;

&lt;!-- class --&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;ch13-text-classification-and-naive-bayes&quot;&gt;CH13 Text Classification and Naive Bayes&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Text Classification&lt;/li&gt;
  &lt;li&gt;Naive Bayes&lt;/li&gt;
  &lt;li&gt;Naive Bayes Theory&lt;/li&gt;
  &lt;li&gt;Evaluation of Text Classification&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;text-classification&quot;&gt;Text Classification&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;standard supervised&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre-define categories and lebel document&lt;/li&gt;
  &lt;li&gt;classify new documents&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;分類的方法&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;人工判斷
 準確但成本高&lt;/li&gt;
  &lt;li&gt;Rule-based
 很多if/else的rule,看到王建民就分類到體育新聞  &lt;br /&gt;
 e.g. google Alert&lt;/li&gt;
  &lt;li&gt;Statistical/Probabilistic
    &lt;ul&gt;
      &lt;li&gt;Instance-based classifiers&lt;br /&gt;
  e.g. kNN&lt;/li&gt;
      &lt;li&gt;Discriminative classifiers&lt;br /&gt;
  學習出分隔的形式(一條線,一棵樹)&lt;br /&gt;
  資料少容易overfit&lt;br /&gt;
  e.g. Decision tree,Neural Network&lt;/li&gt;
      &lt;li&gt;Generative classifier&lt;br /&gt;
  利用大量資料學習出分佈模型(mean,varience)&lt;br /&gt;
  e.g Naive Bayes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;K-Nearest Neighbor Classifier(KNN)&lt;/strong&gt;&lt;br /&gt;
Keep all train data&lt;br /&gt;
優點:不需要training(每一個點都記錄下來)&lt;br /&gt;
缺點:不能做大量資料&lt;/p&gt;

&lt;h2 id=&quot;naive-bayes-classifier&quot;&gt;Naive Bayes Classifier&lt;/h2&gt;
&lt;p&gt;arg max_c 找到一個c(類別)使P(c|d)最大
避免under flow -&amp;gt; 取log連乘變成連加&lt;/p&gt;

&lt;p&gt;機率會有零產生-&amp;gt;避免這種情形全部機率做加1&lt;/p&gt;

&lt;p&gt;估計事前機率 是這個類別的機率和不是這個類別的機率
估計完這些係數training就結束&lt;/p&gt;

&lt;p&gt;P(c|d) 給document判斷是哪個類別
P(c|d) = P(c)P(d|c)/P(d)  分母不考慮,和分類無關
且分子越大越好
P(d|c)可以拆成多個Term的連乘積
且假設每個字之間獨立&lt;/p&gt;

&lt;p&gt;Feture Selection&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reduces training time
挑特定的字訓練模型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two idea&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mutual information
  計算字的交互作用,每一個字會有一個值,找gap最大的切開,拿比較相關的字做訓練&lt;/li&gt;
  &lt;li&gt;CHI-Square statistic
  用機率方法計算,算出機率高的就拿去做訓練資料&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Evaluations
    測試資料和training data不能有overlapping&lt;/p&gt;

&lt;!-- class --&gt;

&lt;hr /&gt;

&lt;!---------------------------- 20170517 --------------------------------------------&gt;
&lt;h1 id=&quot;ch14-vector-space-classification&quot;&gt;CH14 Vector Space Classification&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#ch14_1&quot;&gt;Rocchio classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch14_2&quot;&gt;kNN classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch14_3&quot;&gt;linear classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ch14_4&quot;&gt;Bias-Variance Tradeoff&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;vector-space-classification-基本假設&quot;&gt;Vector Space Classification 基本假設&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;資料如果同類會形成一個連續的空間&lt;/li&gt;
  &lt;li&gt;資料如果不同類別,所形成的空間不會有overlap&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ch14_1&quot;&gt;Rocchio classification(Nearest centroid classifier)&lt;/h2&gt;
&lt;p&gt;將資料表示成向量形式,&lt;br /&gt;
並將各類別的資料計算出重心(所有向量加總取平均),&lt;br /&gt;
利用distance計算相似度&lt;br /&gt;
計算的成本很低,但效果不太好(比naive bayes差)&lt;/p&gt;

&lt;p&gt;主要是沒有處理：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;nonconvex&lt;/li&gt;
  &lt;li&gt;multimodal classes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;應用：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1970年SMART搜索系統中,應用在relevance feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ch14_2&quot;&gt;k Nearest Neighbors(kNN) classification&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;要分類的點根據最近的k個鄰居做投票決定&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;k = 1 過於sensitive,太容易由某一類變成另外一類&lt;br /&gt;
k太大結果過於模糊&lt;br /&gt;
k通常選擇奇數(3,5,7),通常使用heuristic來決定k值&lt;/p&gt;

&lt;p&gt;example:
k = 3
&lt;img src=&quot;/img/websearching/kNN3.png&quot; alt=&quot;kNN = 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;計算相似度:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Euclidean distance&lt;/li&gt;
  &lt;li&gt;Hamming distance (binary instance)&lt;/li&gt;
  &lt;li&gt;cosine similarity of tfidf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特色:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;沒有任何學習&lt;/li&gt;
  &lt;li&gt;如果資料集很大,準確率非常高,資料集小可能就不會很準&lt;/li&gt;
  &lt;li&gt;大致上準確率會較Naive Bayes和Rocchio高&lt;/li&gt;
  &lt;li&gt;Simple, expensive at test time, high variance, non-linear&lt;/li&gt;
&lt;/ul&gt;

&lt;!---------------------------- 20170517 --------------------------------------------&gt;
&lt;!---------------------------- 20170518 --------------------------------------------&gt;

&lt;h2 id=&quot;ch14_3&quot;&gt;Linear classification&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;想法是將所有的資料分為兩類,用一些公式計算出來的值大於一個門檻值分為一類,小於門檻值分到另一類&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum{W_iX_i} &gt; \theta, \quad \theta (threshold)&lt;/script&gt;

&lt;p&gt;線性分類器代表一個分界面,在一維代表一個點、二維代表一條線、三維代表一個平面&lt;/p&gt;

&lt;p&gt;找到分界面的演算法分為兩類&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Simple learning algorithms&lt;br /&gt;
 透過學習資料,學出分界面的參數
    &lt;ul&gt;
      &lt;li&gt;Naive Bayes,Rocchio,kNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Iterative algorithms
    &lt;ul&gt;
      &lt;li&gt;Support vector machines,&lt;a href=&quot;https://www.cs.utexas.edu/~teammco/misc/perceptron/&quot;&gt;Perceptron&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Linear Classifier算是多個方法的集大成&lt;br /&gt;
e.g. Naive Bayes,Perceptron,Rocchio,Logistic regression,Support vector machines(with linear kernel),Linear regression with threshold&lt;/p&gt;

&lt;h2 id=&quot;ch14_4&quot;&gt;Bias-Variance Tradeoff&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;一種量測分類方法的指標&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&quot;&gt;Bias-variance wiki&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bias &lt;br /&gt;
  分類的結果和真實結果差距多少&lt;br /&gt;
  差距越大Bisa越高&lt;/li&gt;
  &lt;li&gt;Variance&lt;br /&gt;
  比較每次猜出來的結果差異會不會很大&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;理想情形是low Bias, low Variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Err(x) = Bias^2 + Variance + Irreducible Error&lt;/script&gt;

&lt;!---------------------------- 20170518 --------------------------------------------&gt;

&lt;!---------------------------- 20170526 --------------------------------------------&gt;
&lt;h1 id=&quot;ch15-support-vector-machines-and-machine-learning-on-documents&quot;&gt;Ch15 Support Vector Machines and Machine Learning on Documents&lt;/h1&gt;

&lt;p&gt;在linear classification中,分界線有無限多條,SVM可以在這些線中找到一條最佳的線,而這條線只會由少數的點來形成。這些點是由分割上最困難的點所成的集合,集合稱作support vector&lt;/p&gt;

&lt;p&gt;要怎麼找到最佳的分割界面(hyperplane)&lt;br /&gt;
SVM是從所有可能的分割界面中找到geometric margin最大的作為分割界面&lt;/p&gt;

&lt;p&gt;Geometric Margin&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;訓練出來的線,兩端碰到最近的support vector中的點所為成的區域&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;目標是找到最大的$\rho$
&lt;img src=&quot;/img/websearching/svm01.png&quot; alt=&quot;svm01&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;w: hyperplane normal vector&lt;/li&gt;
  &lt;li&gt;$x_i$: data point i&lt;/li&gt;
  &lt;li&gt;$y_i$: class of data point (+1 or -1)&lt;/li&gt;
  &lt;li&gt;classifier: f($x_i$) = sign($w^Tx_i$ + b)    //b: bias&lt;/li&gt;
  &lt;li&gt;Define functional margin of $x_i$ is $y_i(w^Tx_i+b)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果分到1類別帶入分類器會$\geq$1 &lt;br /&gt;
分到-1類別帶入分類器會$\leq$-1&lt;/p&gt;

&lt;p&gt;$w^Tx_i + b \geq 1 \quad if\; y_i = 1$&lt;br /&gt;
$w^Tx_i + b \leq -1 \quad if\; y_i = -1$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/websearching/svm02.png&quot; alt=&quot;svm02&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\rho = \frac{2}{\left | w \right |}$&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Quadratic_programming&quot;&gt;quadratic optimization problem&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;找到w和b使$\rho = \frac{2}{\left | w \right |}$最大&lt;br /&gt;
$w^Tx_i + b \geq 1 \quad if\; y_i = 1$&lt;br /&gt;
$w^Tx_i + b \leq -1 \quad if\; y_i = -1$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;較好的表示方法&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;要讓$\rho$最大,最小化$\left | w \right |$&lt;br /&gt;
找到w和b使$\phi(w) = \frac{1}{2} \left | w \right |$最小  &lt;br /&gt;
$y_i(w^Tx_i + b) \geq 1$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;希望$y_i和(w^Tx_i$ + b)同號 &lt;br /&gt;
也就是希望分類結果和帶入分類器結果是同號&lt;/p&gt;

&lt;!---------------------------- 20170526 --------------------------------------------&gt;

&lt;!---------------------------- 20170601 --------------------------------------------&gt;
&lt;p&gt;dual problem&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;每一個線性規劃的問題(primary problem)都有一個對映的線性規劃問題(dual problem),解決dual problem就可以得到原來問題的解&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;解決上面問題的dual problem&lt;br /&gt;
&lt;strong&gt;Lagrange multiplier&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;找到$\alpha_1,…,\alpha_N$使$\sum{\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\vec{x}_i^T\vec{x}_j}$最大&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;$\sum{\alpha_iy_i} = 0$&lt;/li&gt;
    &lt;li&gt;$\alpha_i \geq 0 \quad for \; all \; 1 \leq i \leq N$&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;solution&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;$\vec{w}=\sum\alpha_iy_i\vec{x}_i$  &lt;br /&gt;
$b = y_k-\vec{w}^T\vec{x}_k\quad for\; any\; \vec{x}_k \; such\; that\; \alpha_k \neq 0$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;大多數的點$\alpha_i = 0$代表不重要的點,而非零的$\alpha$代表為support vector&lt;/p&gt;

&lt;p&gt;分類的function&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;$f(\vec{x}) = sign(\sum\alpha_iy_i\vec{x_i}^T\vec{x} + b)$
&lt;!---------------------------- 20170601 --------------------------------------------&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!---------------------------- class --------------------------------------------&gt;
&lt;p&gt;soft margin classification
允許某些點可以分錯
$\phi$(w) 1/2||w|| + C$\sum$ slack
C 越小越可以容錯
因為希望$\phi$(w) 出來的值可以越小越好&lt;/p&gt;

&lt;h4 id=&quot;non-linear-svms&quot;&gt;Non-linear SVMs&lt;/h4&gt;
&lt;p&gt;用一條線分不開的情形,
svm提供kernel function的概念,將點map到更高維的空間中,就有機會能夠分割&lt;/p&gt;

&lt;p&gt;kernel function
$K(x_i,x_j)=x_i^Tx_j$&lt;/p&gt;

&lt;p&gt;透過kernel function可以知道高維空間的內積值&lt;/p&gt;

&lt;h4 id=&quot;common-kernels&quot;&gt;Common kernels&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Linear&lt;/li&gt;
  &lt;li&gt;Polynomial $$&lt;/li&gt;
  &lt;li&gt;Radial basis function(infinite dimensional space)
&lt;!---------------------------- class --------------------------------------------&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>SocialCloudComputing</title>
   <link href="http://localhost:4000/2017/cloudComputing/"/>
   <updated>2017-02-25T00:00:00+08:00</updated>
   <id>http://localhost:4000/2017/cloudComputing</id>
   <content type="html">&lt;h3 id=&quot;outline&quot;&gt;Outline&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Centrality Analysis&lt;/li&gt;
  &lt;li&gt;Community Detection&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;Link Prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;Label Prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5&quot;&gt;Influence maximization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6&quot;&gt;Outbreak Detection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Role/Postion Analysis&lt;/li&gt;
  &lt;li&gt;Social Relation Extraction&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9&quot;&gt;Cloud Computing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h4 id=&quot;journals&quot;&gt;Journals&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nature&lt;/li&gt;
  &lt;li&gt;Science&lt;/li&gt;
  &lt;li&gt;Physical Review&lt;/li&gt;
  &lt;li&gt;Social Networks&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Knowledge Discovery from Data (TKDD)&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Intelligent Systems and Technology(TIST)&lt;/li&gt;
  &lt;li&gt;ACM Transactions on Social Computing(TSC)&lt;/li&gt;
  &lt;li&gt;IEEE Transactions on Knowledge and Data Engineering(TKDE)&lt;/li&gt;
  &lt;li&gt;IEEE Transactions on Computational Social System&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;social-networks&quot;&gt;Social Networks&lt;/h4&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sociocentric&lt;/td&gt;
      &lt;td&gt;Egocentric&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;根據整群分析&lt;/td&gt;
      &lt;td&gt;根據個人分析,向外延伸&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;information Network&lt;br /&gt;
paper reference&lt;br /&gt;
web hyperlink&lt;br /&gt;
Language&lt;/li&gt;
  &lt;li&gt;Social Network&lt;br /&gt;
FB好友關係&lt;/li&gt;
  &lt;li&gt;Technology Network&lt;br /&gt;
電力系統(Power grid)&lt;/li&gt;
  &lt;li&gt;Biologycal Network&lt;br /&gt;
蛋白質互動關係,食物鏈&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;為什麼要分這麼多類Network?&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;因為要分析的點不同,可能在information Network中很重要的,卻在Social Network可能不是那麼重要&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;network-properties&quot;&gt;Network Properties&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;small-world effect
 六度分離理論&lt;br /&gt;
 靠點和點距離關係分析&lt;/li&gt;
  &lt;li&gt;Transitivity
 朋友的朋友很可能也是你朋友&lt;br /&gt;
 &lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%9B%86%E8%81%9A%E7%B3%BB%E6%95%B0&quot;&gt;Clustering Coeffieient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Degree distribution
 Real world network : &lt;br /&gt;
 Power law
    &lt;blockquote&gt;
      &lt;p&gt;P&lt;sub&gt;k&lt;/sub&gt; = CK&lt;sup&gt;-α&lt;/sup&gt;&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Heavy-tailed degree distribution&lt;br /&gt;
 大量很低的數量,集合起來還是很驚人&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Network resilience&lt;br /&gt;
 如果拿掉一些點/邊,連通性會有什麼變化？&lt;br /&gt;
 (e.g.有些人掛了,離職)&lt;br /&gt;
 連接path的長度變長,或是disconnect &lt;br /&gt;
 廣告投放要投在哪個點影響力最大,如果是傳染病隔離哪個點最有效?&lt;/li&gt;
  &lt;li&gt;Mixing patterns&lt;br /&gt;
 探討兩邊節點的type,可能因為什麼關係成為朋友(職業/興趣/文化)&lt;/li&gt;
  &lt;li&gt;Degree Correlations
 觀察兩邊點的degree&lt;br /&gt;
 內向和外向人(朋友多,degree高)觀察&lt;/li&gt;
  &lt;li&gt;Community Structure 
 一群點邊的密度很高,稱作一個community  &lt;br /&gt;
 clique 判斷是否認兩個點是否都有邊相連(clique problem 分團問題)&lt;br /&gt;
 clique problem 是 NP-Complete&lt;br /&gt;
 Connected commponets :有連通的子圖&lt;/li&gt;
  &lt;li&gt;Network motifs    &lt;br /&gt;
在音樂上motifs是一種作曲法,靈感的意思&lt;br /&gt;
 在生物基因上是一些重複的pattern&lt;br /&gt;
 在社群希望找到出現次數較高的motifs(最常出現的subgraph)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;補充資料：&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%B1%B3%E7%88%BE%E6%A0%BC%E5%80%AB%E5%AF%A6%E9%A9%97&quot;&gt;米爾格倫實驗 Milgram experiment 服從威權實驗&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;central-of-network&quot;&gt;Central of Network&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;找到最重要的點(central)&lt;/p&gt;

    &lt;p&gt;local&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Degree&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;global&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;ol&gt;
        &lt;li&gt;Closeness&lt;/li&gt;
        &lt;li&gt;Betweeness&lt;/li&gt;
        &lt;li&gt;Eigenvector&lt;/li&gt;
      &lt;/ol&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Group Centrality&lt;br /&gt;
找到一群最有影響力的人&lt;br /&gt;
在小世界理論中,如果送信到目標的前一步,都是經由特定的3個人,代表這三個人很重要,&lt;br /&gt;
目前social network還無法透過社群網站判斷這些人&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Social actors(群眾的智慧)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Connectors&lt;br /&gt;
認識很多人,很擅長社交&lt;/li&gt;
  &lt;li&gt;Mavens&lt;br /&gt;
資訊專家,知道很多各式訊息&lt;/li&gt;
  &lt;li&gt;Salesman&lt;br /&gt;
容易說服別人,擅長協調&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Social network的四種centrality&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Degree centrality(local)&lt;br /&gt;
點的重要性,若network的規模大小不同,做normalize(除總size-1)&lt;/li&gt;
  &lt;li&gt;Betweeness Centrality&lt;br /&gt;
 A到B的shortest path有幾條經過Node&lt;script type=&quot;math/tex&quot;&gt;_i&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Closeness Centrality
 點i和所有點j的shortest path平均的距離&lt;/li&gt;
  &lt;li&gt;Eigenvector Centrality  &lt;br /&gt;
 這個點的重要性,透過看他朋友點的重要性&lt;br /&gt;
eigenvector
    &lt;blockquote&gt;
      &lt;p&gt;一個向量乘上一個矩陣(transform),方向不變但scale可能會變&lt;br /&gt;
Ax = &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;x&lt;br /&gt;
A矩陣代表social network關係(1:朋友關係,0:不是朋友)&lt;br /&gt;
x代表重要性&lt;br /&gt;
概念類似PageRank,page rank的值是連到他網頁的值加總&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;最短路徑演算法&lt;/strong&gt;&lt;br /&gt;
unweighted graph&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;BFS&lt;/li&gt;
  &lt;li&gt;Floyd-Warshall&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Group centrality&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;找出social network中幾個最有影響力的人&lt;br /&gt;
或指定某幾個人觀察這些人的影響力&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- 20170413  --&gt;
&lt;p&gt;&lt;strong&gt;Properties of cohesion 凝聚力的判斷&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Mutuality of ties&lt;br /&gt;
 所有subgroup彼此都有編相連,在graph中就是完全圖的概念&lt;br /&gt;
 e.g. clique&lt;/li&gt;
  &lt;li&gt;Closeness or reachability of subgroup members&lt;br /&gt;
 不需要直接有邊相連,間接有相連就行了&lt;br /&gt;
 e.g. N-clique,N-clan,N-club&lt;/li&gt;
  &lt;li&gt;Frquency of ties among members&lt;br /&gt;
 Mutuality of ties是說假設有n個人必須要和n-1個人相連,&lt;br /&gt;
 那Frquency of ties among members只需要和n-k個人相連就可以了&lt;br /&gt;
 是Mutuality of ties放寬版本&lt;br /&gt;
 e.g. K-plex,K-core&lt;/li&gt;
  &lt;li&gt;Relative frequency of ties among subgroup members compared to non-member&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Clique&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;maximal complete subgraph,最大的子圖任兩點都有邊相連&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/img/cloudcomputing/community01.png&quot; alt=&quot;clique img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N-clique &lt;br /&gt;
  在grahp中,任兩個點之間的距離&amp;lt;N  &lt;br /&gt;
  e.g. 2-cliques: {1,2,3,4,5},{2,3,4,5,6}&lt;/li&gt;
  &lt;li&gt;N-clan&lt;br /&gt;
  必須是N-clique&lt;br /&gt;
  在subgraph中,任兩個點之間的距離&amp;lt;N&lt;br /&gt;
  e.g. 2-clan: {2,3,4,5,6} &lt;br /&gt;
  (4-&amp;gt;5要經過6,但只考慮1,2,3,4,5這個subgraph)&lt;/li&gt;
  &lt;li&gt;N-club&lt;br /&gt;
  不必是N-clique,但一定要是subgraph of n-cliques  &lt;br /&gt;
  2-clubs: {1,2,3,4},{1,2,3,5},{2,3,4,5,6}&lt;/li&gt;
  &lt;li&gt;K-plex&lt;br /&gt;
  如果是clique每個點的degree是n-1&lt;br /&gt;
  如果是k-plex,每個點的degree是n-k&lt;br /&gt;
  假設subgraph有4個點,2-plex每個點的degree至少是2&lt;/li&gt;
  &lt;li&gt;K-core&lt;br /&gt;
  至少和k個人是朋友
  每個點的degree至少是k&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Community Detection Approaches&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Kernighan-Lin Alog(KL algorithm)&lt;/li&gt;
  &lt;li&gt;Hierarchical Clustering&lt;/li&gt;
  &lt;li&gt;Modularity Maximization&lt;/li&gt;
  &lt;li&gt;Bridge-Cut Algo&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;kl-algorithm&quot;&gt;KL algorithm&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;input: weighted graph&lt;br /&gt;
output: 切成兩個equal-size subgraph,且橫跨兩群的crossing edge &lt;br /&gt;
目的是相望群和群之間差異大,群內部的差異小&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;步驟&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;任意切成兩半&lt;/li&gt;
  &lt;li&gt;計算每一點的difference&lt;/li&gt;
  &lt;li&gt;計算每個邊的gain&lt;/li&gt;
  &lt;li&gt;從gain最大的開始做交換,交換後的點不再考慮(lock)&lt;/li&gt;
  &lt;li&gt;交換到直到全部的點都被lock住&lt;/li&gt;
  &lt;li&gt;挑gain總和最大的就是最終交換結果&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;交換數回合,若遇到gain是負的紀錄下來並繼續嘗試做交換,到最後再找gain最好的
交換完後的點就lock住不進入下一回合&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;external cost&lt;br /&gt;
  crossing edge的cost(連向別群的cost)(cut-size)&lt;/li&gt;
  &lt;li&gt;internal cost&lt;br /&gt;
  連向同群的cost&lt;/li&gt;
  &lt;li&gt;difference&lt;br /&gt;
  external cost - internal cost&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Gain&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;用來評估是否要交換的值
例如a,b屬於不同群,ab做交換&lt;br /&gt;
Gain = &lt;script type=&quot;math/tex&quot;&gt;D_a + D_b - 2\times W_{ab}&lt;/script&gt;&lt;br /&gt;
(Difference a + Difference b - 2*weighted ab)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;若考慮a,b交換&lt;br /&gt;
old cost = &lt;script type=&quot;math/tex&quot;&gt;z + E_a + E_b - W_{ab}&lt;/script&gt; &lt;br /&gt;
new cost = &lt;script type=&quot;math/tex&quot;&gt;z + I_a + I_b + W_{ab}&lt;/script&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;z (與a,b沒有連接的其他crossing edge總和)&lt;br /&gt;
E (external cost)&lt;br /&gt;
I (internal cost)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;KL algorithm複雜度&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt; 找到最適合交換的兩點,有n pair要交換 ⇒ &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt; 
&lt;!-- 20170413  --&gt;&lt;/p&gt;

&lt;!-- 20170512 --&gt;
&lt;h4 id=&quot;hiraichiecal&quot;&gt;Hiraichiecal&lt;/h4&gt;
&lt;p&gt;bottom-up&lt;br /&gt;
每一回合都找兩個最像的做合併&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;single link  &lt;br /&gt;
  距離取min&lt;/li&gt;
  &lt;li&gt;complete link&lt;br /&gt;
  距離取max&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;distance-matrix&quot;&gt;Distance Matrix&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Approach1  &lt;br /&gt;
計算weights W&lt;script type=&quot;math/tex&quot;&gt;_{ij}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;i到j的路徑越多代表i和j關係越好&lt;/li&gt;
      &lt;li&gt;只能找non-overlapped paths&lt;/li&gt;
      &lt;li&gt;只要i到j的路徑都算(weighted by length)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;X&lt;script type=&quot;math/tex&quot;&gt;_{ij}&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{W_{ij}}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Approach 2&lt;br /&gt;
 如果i和j視同一群,那他們有相似的behavior&lt;br /&gt;
 behavoir&lt;br /&gt;
     i和j到commuinty其他點的平均距離相似&lt;/li&gt;
  &lt;li&gt;Approach 3&lt;br /&gt;
 &lt;script type=&quot;math/tex&quot;&gt;\frac{J(i,j)}{min(K_i,K_j)}&lt;/script&gt; &lt;br /&gt;
 看兩個人共同朋友個數,共同朋友越多J(i,j)越大&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;edge-removal-approach&quot;&gt;Edge-removal Approach&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;不斷的拿掉邊(bridge edge),會出現越多的群數,直到符合要的群數&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;betweeness&lt;/strong&gt;&lt;br /&gt;
    一開始想說可以用degree少,但不夠完全&lt;br /&gt;
    在centrality的betweeness是以node考量&lt;br /&gt;
    在這的betweeness是以edge考量&lt;/p&gt;

&lt;h3 id=&quot;gn-algorithm&quot;&gt;GN algorithm&lt;/h3&gt;
&lt;p&gt;top-down(起始是一個commuinity,並分群下去)&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;拿掉betweeness最高的邊 → 重算betweeness → 計算community&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;計算邊的betweeness&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;shortest path&lt;br /&gt;
 任兩點最短路徑有多少條會經過邊&lt;/li&gt;
  &lt;li&gt;Random-walk 
 計算a會走到b的機率&lt;br /&gt;
 a走到b會經過邊v的機率&lt;/li&gt;
  &lt;li&gt;Current-flow
 引進電路學概念的計算方法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;缺點&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;計算最短路徑耗時
 O(m^2n)&lt;br /&gt;
 m edge (O(mn)betweeness)&lt;/li&gt;
  &lt;li&gt;什麼時候停?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;改善&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Partial betweeness (Apprximation)&lt;br /&gt;
Randomly sampled by Monte Carlo Estimate&lt;/li&gt;
  &lt;li&gt;Edge clustering coefficient
  coefficient越高代表關係越好
the smaller coefficient the higher betweeness&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;modularity&quot;&gt;Modularity&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Modularity measure:
    how good a particular partition forms a community.
    評估community切分的好不好  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;U 看internal edge的比例&lt;br /&gt;
R 平均i和j會有邊的機率(期望值)      &lt;br /&gt;
Q = U - R&lt;/p&gt;

&lt;p&gt;Q = 0 no community&lt;br /&gt;
Q ~ 1 prefect cut&lt;/p&gt;

&lt;h3 id=&quot;newman-fast-alogorithm&quot;&gt;Newman Fast Alogorithm&lt;/h3&gt;
&lt;p&gt;利用hireachcal合併,並每個步驟算modurity,並找出最高的Q做切分&lt;/p&gt;

&lt;h3 id=&quot;bridge-cut&quot;&gt;Bridge cut&lt;/h3&gt;
&lt;p&gt;integrity一致性  &lt;br /&gt;
N(v)&lt;br /&gt;
d(v): degree of node&lt;br /&gt;
Density&lt;br /&gt;
Direct neighbor subgraph of v&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clustering coeffiecient&lt;/strong&gt;   &lt;br /&gt;
	觀察v的鄰居的朋友關係&lt;br /&gt;
	例如v有4個朋友,那4個人最多有6個關係,算關係的比例&lt;br /&gt;
	實際上有關係/最多有幾個關係&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bridge Centrality&lt;/strong&gt;&lt;br /&gt;
	rank of betweenness centrality * rank of bridging coeffiecient&lt;br /&gt;
	如果只考慮betweeness(global)會有一些情況不太好  &lt;br /&gt;
	加入bridge centrality可以考慮到local的特質&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Community Search&lt;/strong&gt;&lt;br /&gt;
    給一個social network,並給一些query(其中幾個人),&lt;br /&gt;
    given grahp G, a set of query node&lt;br /&gt;
    goal: find a densely subgraph of G, and contains the query nodes&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Induced Subgraph&lt;/strong&gt;&lt;br /&gt;
xy edge在G中,xy edge也要在induce subgraph中&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;goodness function&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;edge degree
 時間複雜度太大&lt;/li&gt;
  &lt;li&gt;average degree&lt;/li&gt;
  &lt;li&gt;minumin degree
 這群人認識最少的人,讓這個人的值變大
 induced subgraph的degree&lt;br /&gt;
 容易受到outlier影響&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Constrain&lt;/strong&gt;  &lt;br /&gt;
distance constrain&lt;br /&gt;
限制邀請來的人的最長距離&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monotone Function&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monotone increaing&lt;/li&gt;
  &lt;li&gt;monotone decresing&lt;/li&gt;
  &lt;li&gt;non-monotone
&lt;!-- 20170512 --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;!-- 20170421 --&gt;
&lt;blockquote&gt;

  &lt;ul&gt;
    &lt;li&gt;Link Prediction&lt;/li&gt;
    &lt;li&gt;Node-wise Similarity Based Methods&lt;/li&gt;
    &lt;li&gt;Topological Pattern Based Methods&lt;/li&gt;
    &lt;li&gt;Probabilistic Model Based Methods&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3&quot;&gt;Link Prediction &lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Predict the existence of links&lt;/li&gt;
  &lt;li&gt;Predict the type of links&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Strategies of Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Knowledge-driven strategy&lt;br /&gt;
 專家系統(領域專家提供rule)&lt;/li&gt;
  &lt;li&gt;Data-driven approach&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;problem&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Link existence prediction&lt;br /&gt;
 邊是否存在&lt;/li&gt;
  &lt;li&gt;Link classification&lt;br /&gt;
 關係的總類&lt;/li&gt;
  &lt;li&gt;Link regression&lt;br /&gt;
 最重要的邊是哪一個&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Web hyperlink creation&lt;/li&gt;
  &lt;li&gt;Collaborative filitering&lt;/li&gt;
  &lt;li&gt;Information retrieval&lt;/li&gt;
  &lt;li&gt;Clustering&lt;/li&gt;
  &lt;li&gt;Record linkage&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;node-wise-similarity-based-method&quot;&gt;Node-wise Similarity Based Method&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;計算兩個點的相似度,如果兩個點很相似他們可能就有link&lt;br /&gt;
e.g. Similarity between words&lt;br /&gt;
觀察word的前後文字來判斷相似程度&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Learning-Based Similarity Measure&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Binary Classification Approach
    &lt;ul&gt;
      &lt;li&gt;Decision Tree&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Regression-based Approach(回歸)  &lt;br /&gt;
  e.g linear regression&lt;br /&gt;
  &lt;script type=&quot;math/tex&quot;&gt;Y = \alpha + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n&lt;/script&gt;&lt;br /&gt;
  利用學習方式估計出&lt;script type=&quot;math/tex&quot;&gt;\alpha , \beta&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;topological-pattern-based-methods&quot;&gt;Topological Pattern Based Methods&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;計算兩點之間的分數,若大於某個值就表示他們之間有關係(連線)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Local Method&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Common Neighbors(CN)  &lt;br /&gt;
  計算共同的鄰居&lt;/li&gt;
      &lt;li&gt;Salton Index&lt;br /&gt;
  類似cosine similiary&lt;/li&gt;
      &lt;li&gt;Jaccard Coefficient(JC)&lt;br /&gt;
  交集/聯集&lt;/li&gt;
      &lt;li&gt;Leicht-Holme-Newman Index(LHN)&lt;/li&gt;
      &lt;li&gt;Hub Promoted Index(HPI)&lt;br /&gt;
  Hub概念像是入口網站,類似目錄連到很多子分支&lt;/li&gt;
      &lt;li&gt;Hub Depres Index(HDI)&lt;/li&gt;
      &lt;li&gt;Adamic/Adar(AA) &lt;br /&gt;
  x和y是朋友的分數是x和y的共同朋友的鄰居-x和y 倒數總和&lt;/li&gt;
      &lt;li&gt;Resource Allocation Index(RA)&lt;br /&gt;
  和AA差在分母沒有取log&lt;/li&gt;
      &lt;li&gt;Preferential Attachment&lt;br /&gt;
  x和y是朋友的分數就是x的鄰居乘上y鄰居&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Performance: RA &amp;gt; AA &amp;gt; CN &amp;gt; … &amp;gt; PA&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;global Method&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;katz&lt;br /&gt;
  看x到y距離是1,2,3..n的path有幾條,乘上一個參數做加總&lt;/li&gt;
      &lt;li&gt;Hitting Time&lt;br /&gt;
  x走到y,做random work的期望值作為比較條件&lt;/li&gt;
      &lt;li&gt;PageRank&lt;/li&gt;
      &lt;li&gt;SimRank&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;probabilistic-model-based-methods&quot;&gt;Probabilistic Model Based Methods&lt;/h4&gt;
&lt;p&gt;e.g. relational Markov model&lt;br /&gt;
&lt;!-- 20170421 --&gt;&lt;/p&gt;

&lt;!-- 20170420 --&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;4&quot;&gt;Labeld Social Network &lt;/h1&gt;
&lt;h4 id=&quot;type-of-labels&quot;&gt;Type of Labels&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Binary&lt;/li&gt;
  &lt;li&gt;Numeric&lt;/li&gt;
  &lt;li&gt;cate&lt;/li&gt;
  &lt;li&gt;text-free&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;label-prediction&quot;&gt;Label Prediction&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;根據已知的label預測未知點的label&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;Inference vs. Learning&lt;br /&gt;
 Inference(unsupervised)&lt;br /&gt;
 Learning(supervised)&lt;/li&gt;
  &lt;li&gt;Disjoint vs. Collective&lt;br /&gt;
 Disjoint
 沒有標籤的點就不考慮&lt;br /&gt;
 Collective&lt;br /&gt;
 沒有標籤的點也會放進去考慮&lt;/li&gt;
  &lt;li&gt;Across-network vs. within-network learning&lt;br /&gt;
 Across-network拿一個social network model去預測另一個social network&lt;br /&gt;
 within-network拿全部資料做的model來做預測&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;clues-to-predict-labels&quot;&gt;Clues to Predict Labels&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Label Independent approaches 特徵值沒有用到label的訊息
    &lt;ol&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
 Attribute of node i (年紀)
 Label of node i (身份)&lt;/li&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
 Network Structures of node i (between centrality)
 Label of node i&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Label Dependent Approaches
    &lt;ol&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
 知道鄰居來預設未知,用其他點來預測未知點&lt;/li&gt;
      &lt;li&gt;Correlation between&lt;br /&gt;
 利用unlabel點來預測&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relational-neighbor-classifier&quot;&gt;Relational Neighbor Classifier&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;看鄰居多數是什麼就判斷node是什麼&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;問題：&lt;br /&gt;
    如果已知的點很少,unknown的很多(Sparse label),若只用一個點就判斷就沒那麼可靠&lt;br /&gt;
解決：
    Iterative Relation Neighbor classifier  &lt;br /&gt;
    判斷分為好幾回合,若多數點是unknown那就判斷是unknown &lt;br /&gt;
    unknown也是一種label&lt;br /&gt;
&lt;!-- 20170420 --&gt;&lt;/p&gt;

&lt;!-- 20170512 --&gt;
&lt;h2 id=&quot;ghost-edges-for-node-label-prediction&quot;&gt;Ghost Edges for Node Label Prediction&lt;/h2&gt;
&lt;p&gt;將一些不是直接連接的node但有影響力的點,用ghost edge連起來&lt;br /&gt;
那怎麼判斷點的重要性 → Random walk with Restart&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;有一定的機率會跳到起點&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;steady-state-probability&quot;&gt;Steady-state Probability&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Markov Process  &lt;br /&gt;
  e.g. 城市和郊區遷移問題&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;利用Random walk with restart計算所有點對某點的影響力&lt;br /&gt;
在對這些影響力做等級劃分,依照機率分為ABCDEF…等級&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two Classifers&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;GhostEdgeNL&lt;/li&gt;
  &lt;li&gt;GhostEdgeL&lt;br /&gt;
 Logistic regression&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;information-diffusion&quot;&gt;Information Diffusion&lt;/h2&gt;
&lt;p&gt;消息/疾病擴散&lt;br /&gt;
想知道擴散方向,可以擴散到哪等等問題&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 elements of diffusion process&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Senders&lt;/li&gt;
  &lt;li&gt;Receivers&lt;/li&gt;
  &lt;li&gt;Medium(channel)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Types of information Diffusion&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;herd behavior(global information)&lt;br /&gt;
  群眾行為,大多數人怎麼做,就有可能會跟著做&lt;/li&gt;
  &lt;li&gt;information cascades(local information)&lt;/li&gt;
  &lt;li&gt;diffusion of innovation&lt;/li&gt;
  &lt;li&gt;eqidemics&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Diffusion Models&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Descriptive models
    &lt;blockquote&gt;
      &lt;p&gt;機率模型&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Operational models
    &lt;blockquote&gt;
      &lt;p&gt;一步一步的擴散&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Each node can be Active / Inactive&lt;br /&gt;
 Assumption:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;node can switch from inactive to active&lt;/li&gt;
      &lt;li&gt;cannot switch from active to inactive&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;e.g.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Linear Threshold Model&lt;/li&gt;
      &lt;li&gt;Independent Cascade Model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Linear Threshold Model&lt;/strong&gt;  &lt;br /&gt;
每個人都會有一個threshold代表會變成active門檻值  &lt;br /&gt;
每個邊上會有影響力的值(可以是單向或雙向)  &lt;br /&gt;
如果你的鄰居加總的影響力大於你本身的threshold那你也會變成active&lt;br /&gt;
由Senders開始一步一步的擴散&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Independent Cascade Model&lt;/strong&gt;&lt;br /&gt;
每一個人只能影響鄰居一次,失敗了不能再影響一次&lt;br /&gt;
邊上是影響成功的機率&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5&quot;&gt;Influence Maximization Problem&lt;/h1&gt;
&lt;p&gt;給一些起始的senders觀察最後有哪些人被影響&lt;br /&gt;
給k個senders並且找出這k個senders是誰且最後影響的人數最多&lt;br /&gt;
(應用: 廣告要放在哪裡)&lt;br /&gt;
Constrained optimization problem&lt;/p&gt;

&lt;p&gt;問題難度: NP-hard &lt;br /&gt;
證明這個問題的難度:&lt;br /&gt;
    符合Submodular Function,且要在所有node中找出k個,使得f(k)為maximized&lt;br /&gt;
    已經被證明為NP-hard問題&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Submodular Function&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Non-negative&lt;/li&gt;
  &lt;li&gt;Monotone&lt;/li&gt;
  &lt;li&gt;Submodular 邊際效應遞減
    &lt;blockquote&gt;
      &lt;p&gt;f(a,b,v) - f(a,b) &amp;gt; f(a,b,c,v) - f(a,b,c)&lt;br /&gt;
  原來有a,b加入v所增加的量大於原來有a,b,c加入v所增加的量&lt;br /&gt;
  例如原本能考60分,讀一天可以考到80分,&lt;br /&gt;
  和原來能考90分,可是讀一天只能考到95分&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;遇到NP-hard問題考慮的解法&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Approximation Approach&lt;/li&gt;
  &lt;li&gt;平行運算&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Approximation Approach&lt;/strong&gt;&lt;br /&gt;
Greedy algorithm&lt;br /&gt;
每回合找出Submodular最大的作為sender&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;6&quot;&gt;Outbreak Detection&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;能不能透過放sensor提早知道消息的擴散&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;給一個network G(V,E),找到placement A(sensor)
goal : to max R(A)
R(A) : reward&lt;br /&gt;
c(A) : cost&lt;/p&gt;

&lt;p&gt;placement objective&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;detection likelihood&lt;br /&gt;
  希望所有事件都偵測到&lt;/li&gt;
  &lt;li&gt;detection time&lt;br /&gt;
  多久偵測到&lt;/li&gt;
  &lt;li&gt;Population affected
  已經擴散多少,多少人知道&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Approached of Outbreak Detection&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Heuristic in simple case
 each node has equal cost&lt;br /&gt;
 每次都加入一個sensor,那計算每一個node加入的邊際效應&lt;br /&gt;
 並選最大的邊際效應加入&lt;/li&gt;
  &lt;li&gt;Heuristic in more complex case&lt;br /&gt;
多考慮cost,所以算法變成每次加入最大的benifit/cost&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;team-formation-in-social-networks&quot;&gt;Team Formation in Social Networks&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;如何要找到正確的人,組成一個團隊&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Task&lt;/li&gt;
  &lt;li&gt;Expert&lt;/li&gt;
  &lt;li&gt;Network&lt;/li&gt;
  &lt;li&gt;Effective&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;set of n individuals , a graph , a task&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Find&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;必須包含task,individuals skill聯集要包含task&lt;br /&gt;
    communication cost要最小越好&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Communication Cost&lt;/strong&gt;&lt;br /&gt;
Diameter&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;distance     : shortest path (geodesic distance)&lt;/li&gt;
  &lt;li&gt;eccentricity : 其中一個點到其他所有點的shortest path,並取最大的值&lt;/li&gt;
  &lt;li&gt;radius       : minimun eccentrictity 溝通成本最低&lt;/li&gt;
  &lt;li&gt;diameter     : maximun eccentrictity 溝通成本最高&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;measure communication cost&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;diameter&lt;/li&gt;
  &lt;li&gt;minimum spanning tree&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不管用diameter或是MST都是NP-hard&lt;br /&gt;
那縮小範圍可以考慮到加入限制 : &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\quad&lt;/script&gt;人數越少越好 → set cover problem(NP-complete)&lt;/p&gt;

&lt;p&gt;作者提出的方法將set cover problem視為baseline作為比較&lt;br /&gt;
NPC問題通常找近似解,所以需要一個比較的方法&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;RarestFirst Algo for Diameter-TF&lt;br /&gt;
 先從skill少的加入集合考慮,必找diameter低的持續加入集合&lt;br /&gt;
 假設skill少的同時有2個人會,那要挑誰？
    &lt;ol&gt;
      &lt;li&gt;隨便挑&lt;/li&gt;
      &lt;li&gt;從欠缺能力看,計算和有欠缺技能的node計算diameter,挑minimun diameter&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Enhanced Steiner ALog. for MST-TF&lt;br /&gt;
 The Minimum Stener Tree Problem (NP-hard)&lt;br /&gt;
     給入require的點,必找出含有這些點的tree&lt;br /&gt;
     如果require是所有的點,問題就會變成minimal spanning tree,&lt;br /&gt;
     minimal spanning tree是minimum stener tree的speical case&lt;br /&gt;
     如果require只有兩個點,那就會變成shortest path的問題&lt;/p&gt;

    &lt;p&gt;將每一個skill,各至成為一個node,且連到擁有這些技能的點,&lt;br /&gt;
 將題目轉為stener alog,require就是自己建立的node,找到後再去除這些node&lt;br /&gt;
 1.從自己建立的node找一個點放到set中&lt;br /&gt;
 2.計算和require和set中的距離,取最短,並將path上的點加入set,直到所有require都在set中 &lt;br /&gt;
&lt;!-- 20170512 --&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 20170525 --&gt;
&lt;h2 id=&quot;herd-從眾&quot;&gt;Herd 從眾&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;有一群人做決策,大家會趨向一個方向&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Information Diffusion
    &lt;ul&gt;
      &lt;li&gt;Explicit Network
        &lt;ul&gt;
          &lt;li&gt;Global Information Herd Behavior&lt;/li&gt;
          &lt;li&gt;Local Information &lt;br /&gt;
  Information Cascases (e.g. Facebook)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Implicit Network
        &lt;ul&gt;
          &lt;li&gt;Diffusion of Innovations (e.g. ptt movies)&lt;/li&gt;
          &lt;li&gt;Epidemics (e.g. 疾病的擴散)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;herd-example&quot;&gt;Herd example&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Soloman Asch Experiment&lt;br /&gt;
 左邊一個長條圖形和右邊有三個不同長短的長條圖形,比較長度和哪個最相近&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Urn Experiment (Bayesian Modeling of Herd Behavior)&lt;br /&gt;
 有多個學生來猜測甕裡面主要是放什麼顏色的珠寶&lt;br /&gt;
 甕裡面珠寶有紅色和藍色,不會是全部是藍色或全部是紅色&lt;br /&gt;
 每個學生從甕中抓一把,根據自己手中的內容和黑板上紀錄結果做預測&lt;br /&gt;
 並將預測結果寫在黑板上,後面的同學可以參考&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;9&quot;&gt;Cloud computing&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#9_1&quot;&gt;MapReduce&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9_2&quot;&gt;Pig Programming&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf&quot;&gt;The NIST Definition of Cloud Computing&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Parallel&lt;/li&gt;
  &lt;li&gt;Distribute&lt;/li&gt;
  &lt;li&gt;Grid&lt;/li&gt;
  &lt;li&gt;Utility Computing&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;parallel-computing&quot;&gt;Parallel Computing&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;將大問題拆成小問題且同時執行&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;distributed-computing&quot;&gt;Distributed Computing&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;多台獨立電腦透過網路連結且為同一個任務工作&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;grid-computing&quot;&gt;Grid Computing&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;是一種Distributed Computing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;utility-computing&quot;&gt;Utility Computing&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;讓計算像使用電力,水等資源,只要有一個client端就可以使用,不在本地端計算,而是在提供服務的地方計算&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;performance-optimization&quot;&gt;Performance Optimization&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Parallel Computing&lt;/li&gt;
  &lt;li&gt;Job Scheduling&lt;/li&gt;
  &lt;li&gt;Load Balancing&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;survice-models&quot;&gt;Survice Models&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;IaaS(Infrastructure as a Service)
    &lt;blockquote&gt;
      &lt;p&gt;包含硬體,OS,driver,networking&lt;br /&gt;
使用者不用管理或控制底層的東西&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Virtualization:&lt;br /&gt;
 Abstraction of logical resource away from underlying phyical resources&lt;br /&gt;
 Improve utilization,security&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Paas(Platform as a Service)
    &lt;blockquote&gt;
      &lt;p&gt;提供工程師一個寫程式的環境,包含程式語言和工具,且不用管理底層的硬體但可以做控制&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;e.g. Programming IDE,Programming API,System Control interface,Hadoop,Google App Engine,Microsoft Windows Azure&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;SaaS(Software as a Service)
    &lt;blockquote&gt;
      &lt;p&gt;提供應用程式給一般人使用,透過clinet界面就可以使用&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;e.g. web Service,Google App&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;!-- 20170525 --&gt;

&lt;!-- class --&gt;
&lt;h1 id=&quot;9_1&quot;&gt;MapReduce&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;A programming model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;local aggregation
map跑完的結果先處理(先加起來)
可以提升效能,在sort and shuffle可以減少計算&lt;/p&gt;

&lt;p&gt;3 approahes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Combiner&lt;/li&gt;
  &lt;li&gt;Improved Word Count
&lt;!-- class --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- class --&gt;

&lt;p&gt;relational database 如何做join
透過主鍵連結兩張表,map做的事就是找到對應的key和value
key就設為主鍵,但value必須要有紀錄資料和資料來自哪張table
在reduce時才不會出錯&lt;/p&gt;

&lt;p&gt;MapReduce Algorithm Design
提供紀錄中間狀態,提供需要for loop的程式執行
可以自己設定key&lt;/p&gt;

&lt;!-- class --&gt;

&lt;!-- class --&gt;
&lt;p&gt;介紹dijkstra
那要怎麼把dijkstra做成mapreduce版本
dijkstra每一回合都要找到最小cost當成下一點
繼續遞迴的以cost最小的展開下去&lt;/p&gt;

&lt;p&gt;mapreduce可以找到local的最小值
但不能找到global的最小值
mapreduce沒有提供share memory的機制去交換global的資料&lt;/p&gt;

&lt;p&gt;先考慮unweight shortest path
每個edge都看成是1
那目標就變成找到最少的edge數&lt;/p&gt;

&lt;p&gt;那就可以利用BFS解決這個問題&lt;/p&gt;

&lt;p&gt;要將dijksta的問題改成mapreduce版本就要利用這個概念&lt;/p&gt;

&lt;p&gt;平行shortest path algorithm&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;brute force approach&lt;/li&gt;
  &lt;li&gt;all edge have unit distance&lt;/li&gt;
  &lt;li&gt;Parallel BFS
    &lt;ul&gt;
      &lt;li&gt;iterative algorithm&lt;/li&gt;
      &lt;li&gt;每次的結果存到disk,下次再讀取,但會有disk IO 問題&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通常在mapreduce中的資料結構是以linklist的形式儲存
如果是sparse的矩陣會儲存太多的0,在map時會紀錄太多0
所以利用linklist來儲存,有值才紀錄
且對sort and shuffle的步驟負擔會比較小&lt;/p&gt;

&lt;p&gt;每一的mapreduce會做一個hop(層),
且每回合的結果要寫回disk供下一回合使用&lt;/p&gt;

&lt;p&gt;Inverted Files
利用linklist紀錄每個term出現在哪些文章中
&lt;!-- class --&gt;&lt;/p&gt;

&lt;!-- 20170602 --&gt;
&lt;h1 id=&quot;9_2&quot;&gt;Pig Programming&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;sql-like language, 透過hadoop處理大量半結構化資料&lt;/li&gt;
  &lt;li&gt;hadoop sub-project&lt;/li&gt;
  &lt;li&gt;data flow language&lt;/li&gt;
  &lt;li&gt;higher-level language&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;收到user query後，pig會將query轉換成logical plan並在過程中做最佳化,logical plan之後產生physical plan(存取記憶體等操作),physical plan最後轉換成map reduce plan&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;lazy execution&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;直到執行request output(store/dump)才開始執行程式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;好處&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;in memory pipelining  &lt;br /&gt;
  將執行完的結果直接傳給下個指令使用,不需要存到disk再去讀取&lt;/li&gt;
  &lt;li&gt;filter re-ordering across multiple commands  &lt;br /&gt;
  可能先做filter再開始處理join等等的指令,可以減少計算的複雜度(指令的最佳化)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;data type&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;int&lt;/li&gt;
  &lt;li&gt;long&lt;/li&gt;
  &lt;li&gt;float&lt;/li&gt;
  &lt;li&gt;double&lt;/li&gt;
  &lt;li&gt;chararray&lt;/li&gt;
  &lt;li&gt;bytearray&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;指令&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;load                                                &lt;br /&gt;
  load ‘user.txt’;&lt;/li&gt;
  &lt;li&gt;store
  store result into ‘output’&lt;/li&gt;
  &lt;li&gt;dump&lt;/li&gt;
  &lt;li&gt;filter&lt;br /&gt;
  filter User by age &amp;gt; 18 and age &amp;lt;= 25&lt;/li&gt;
  &lt;li&gt;join
    &lt;ul&gt;
      &lt;li&gt;outer join (right / left)&lt;br /&gt;
  filter後濾掉的資料也會出現,以right或left為主&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;group&lt;br /&gt;
  group user_url by url    &lt;br /&gt;
  將欄位一樣的合併在一起&lt;br /&gt;
  pig允許complex data type(一格有多筆資料)&lt;/li&gt;
  &lt;li&gt;cogroup&lt;br /&gt;
  cogroup Users by userId, Urls by user;&lt;br /&gt;
  可以針對多個資料表做group&lt;/li&gt;
  &lt;li&gt;foreach&lt;br /&gt;
  foreach urlgroup generate group as url,&lt;br /&gt;
  count(user_url) as count,&lt;/li&gt;
  &lt;li&gt;order&lt;br /&gt;
  ASC,DESC&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- 20170602 --&gt;

</content>
 </entry>
 

</feed>
